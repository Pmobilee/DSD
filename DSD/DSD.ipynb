{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Code\\Thesis\\DSD/models/cin256_retrained.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paper\\miniconda3\\envs\\D-SD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in x0-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 16, 16) = 768 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loading model from c:\\Code\\Thesis\\DSD/models/cin256_retrained.pt\n",
      "LatentDiffusion: Running in x0-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 16, 16) = 768 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import self_distillation\n",
    "import distillation\n",
    "import saving_loading\n",
    "import generate\n",
    "import wandb\n",
    "import util\n",
    "import os\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from IPython.display import display, clear_output\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.util import *\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import wandb\n",
    "import math\n",
    "import traceback\n",
    "from pytorch_fid import fid_score\n",
    "import shutil\n",
    "import util\n",
    "import saving_loading\n",
    "import generate\n",
    "\n",
    "gradient_updates = 4000\n",
    "lr = 0.0001\n",
    "steps = 8\n",
    "step_scheduler = \"naive\"\n",
    "x0 = True\n",
    "warmup_epochs = 1000\n",
    "cwd = os.getcwd()\n",
    "\n",
    "model_path=f\"{cwd}/models/cin256_retrained.pt\"\n",
    "config_path = f\"{cwd}/models/configs/cin256-v2-custom_x0.yaml\"\n",
    "\n",
    "student, sampler_student = util.create_models(config_path, model_path, student=False)\n",
    "original, sampler_original = util.create_models(config_path, model_path, student=False)\n",
    "optimizer, scheduler = util.get_optimizer(sampler_student, iterations=gradient_updates, warmup_epochs=warmup_epochs,lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDiffusion' object has no attribute 'model_ema'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     42\u001b[0m     student\u001b[39m.\u001b[39muse_ema \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[39mwith\u001b[39;00m student\u001b[39m.\u001b[39mema_scope(): \n\u001b[0;32m     44\u001b[0m             \u001b[39mif\u001b[39;00m x0:\n\u001b[0;32m     45\u001b[0m                 sc\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paper\\miniconda3\\envs\\D-SD\\lib\\contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Code\\Thesis\\DSD\\ldm\\models\\diffusion\\ddpm_x0.py:193\u001b[0m, in \u001b[0;36mDDPM.ema_scope\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mema_scope\u001b[39m(\u001b[39mself\u001b[39m, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    192\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[1;32m--> 193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_ema\u001b[39m.\u001b[39mstore(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m    194\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_ema\u001b[39m.\u001b[39mcopy_to(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m    195\u001b[0m         \u001b[39mif\u001b[39;00m context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Paper\\miniconda3\\envs\\D-SD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LatentDiffusion' object has no attribute 'model_ema'"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 1000\n",
    "ddim_steps_student = steps # Setting the number of steps for the student model\n",
    "ddim_eta = 0.0 # Setting the eta value to 0.0 means a deterministic output given the original noise, essential\n",
    "# For both the student and the original model, the number of steps is set to the same value. \n",
    "# Technically the original model does not need to be trained, but it is kept for comparison purposes.\n",
    "sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "ddim_eta = 0.0 # Setting the eta value to 0.0 means a deterministic output given the original noise, essential\n",
    "scale = 3.0 # This is $w$ in the paper, the CFG scale. Can be left static or varied as is done occasionally.\n",
    "criterion = nn.MSELoss() \n",
    "\n",
    "instance = 0 # Actual instance of student gradient updates\n",
    "generation = 0 # The amount of final-step images generated\n",
    "averaged_losses = []\n",
    "all_losses = []\n",
    "\n",
    "if step_scheduler == \"iterative\": # Halve the number of steps from start to 1 with even allocation of gradient updates\n",
    "    halvings = math.floor(math.log(ddim_steps_student)/math.log(2))\n",
    "    updates_per_halving = int(gradient_updates / halvings)\n",
    "    step_sizes = []\n",
    "    for i in range(halvings):\n",
    "        step_sizes.append(int((steps) / (2**i)))\n",
    "    update_list = []\n",
    "    for i in step_sizes:\n",
    "        update_list.append(int(updates_per_halving / int(i/ 2))) # /2 because of 2 steps per update\n",
    "elif step_scheduler == \"naive\": # Naive approach, evenly distribute gradient updates over all steps\n",
    "    step_sizes=[ddim_steps_student]\n",
    "    update_list=[gradient_updates // int(ddim_steps_student / 2)] # /2 because of 2 steps per update\n",
    "elif step_scheduler == \"gradual_linear\": # Gradually decrease the number of steps to 1, with even allocation of gradient updates\n",
    "    step_sizes = np.arange(steps, 0, -2)\n",
    "    update_list = ((1/len(np.append(step_sizes[1:], 1)) * gradient_updates / np.append(step_sizes[1:], 1))).astype(int) * 2 # *2 because of 2 steps per update\n",
    "elif step_scheduler == \"gradual_exp\": # TEMPORARY VERSION, to test if focus on higher steps is better, reverse of the one below\n",
    "    step_sizes = np.arange(64, 0, -2)\n",
    "    update_list = np.exp((1 / np.append(step_sizes[1:],1))[::-1]) / np.sum(np.exp((1 / np.append(step_sizes[1:],1))[::-1]))\n",
    "    update_list = (update_list * gradient_updates /  np.append(step_sizes[1:],1)).astype(int) * 2 # *2 because of 2 steps per update\n",
    "# elif step_scheduler == \"gradual_exp\": # Exponential decrease in number of gradient updates per step\n",
    "#     step_sizes = np.arange(64, 0, -2)\n",
    "#     update_list = np.exp(1 / np.append(step_sizes[1:],1)) / np.sum(np.exp(1 / np.append(step_sizes[1:],1)))\n",
    "#     update_list = ((update_list * 2) * gradient_updates /  np.append(step_sizes[1:],1)).astype(int)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # student.use_ema = True\n",
    "    # with student.ema_scope(): \n",
    "            if x0:\n",
    "                sc=None\n",
    "            else:\n",
    "                sc = student.get_learned_conditioning({student.cond_stage_key: torch.tensor(1*[1000]).to(student.device)}) # Get the learned conditioning\n",
    "            for i, step in enumerate(step_sizes): # For each step size\n",
    "                # if instance != 0 and \"gradual\" not in step_scheduler:   # Save the model after every step size. Given the large model size, \n",
    "                #                                                         # the gradual versions are not saved each time (steps * 2 * 4.7gb is a lot!)\n",
    "                #     util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "                updates = int(step / 2) # We take updates as half the step size, because we do 2 steps per update\n",
    "                generations = update_list[i] # The number of generations has been determined earlier\n",
    "                print(\"Distilling to:\", step)\n",
    "                \n",
    "                with tqdm.tqdm(torch.randint(0, NUM_CLASSES, (generations,))) as tepoch: # Take a random class for each generation\n",
    "\n",
    "                    for i, class_prompt in enumerate(tepoch):\n",
    "                        generation += 1\n",
    "                        losses = []       \n",
    "                        scale = 3.0\n",
    "                        #scale = np.random.uniform(1.0, 4.0) # Randomly sample a scale for each generation, optional\n",
    "                        c_student = student.get_learned_conditioning({student.cond_stage_key: torch.tensor([class_prompt]).to(student.device)}) # Set to 0 for unconditional, requires pretraining\n",
    "                        \n",
    "                        samples_ddim= None # Setting to None will create a new noise vector for each generation\n",
    "                        predictions_temp = []\n",
    "                        \n",
    "                        for steps in range(updates):\n",
    "                            # with autocast() and torch.enable_grad(): # For mixed precision training, should not be used for final results\n",
    "                                with torch.enable_grad():\n",
    "                                        instance += 1\n",
    "                                        \n",
    "                                        optimizer.zero_grad()\n",
    "                                        samples_ddim, pred_x0_student, _, at= sampler_student.sample_student(S=1,\n",
    "                                                                            conditioning=c_student,\n",
    "                                                                            batch_size=1,\n",
    "                                                                            shape=[3, 64, 64],\n",
    "                                                                            verbose=False,\n",
    "                                                                            x_T=samples_ddim, # start noise or teacher output\n",
    "                                                                            unconditional_guidance_scale=scale,\n",
    "                                                                            unconditional_conditioning=sc, \n",
    "                                                                            eta=ddim_eta,\n",
    "                                                                            keep_intermediates=False,\n",
    "                                                                            intermediate_step = steps*2,\n",
    "                                                                            steps_per_sampling = 1,\n",
    "                                                                            total_steps = ddim_steps_student)\n",
    "                                        \n",
    "                                        # Code below first decodes the latent image and then reconstructs it. This is not necessary, but can be used to check if the latent image is correct\n",
    "                                        # decode_student = student.differentiable_decode_first_stage(pred_x0_student)\n",
    "                                        # reconstruct_student = torch.clamp((decode_student+1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        \n",
    "                                        \n",
    "\n",
    "                                        with torch.no_grad():\n",
    "                                            samples_ddim.detach()\n",
    "                                            samples_ddim, _, _, pred_x0_teacher, _ = sampler_student.sample(S=1,\n",
    "                                                                        conditioning=c_student,\n",
    "                                                                        batch_size=1,\n",
    "                                                                        shape=[3, 64, 64],\n",
    "                                                                        verbose=False,\n",
    "                                                                        x_T=samples_ddim, # output of student\n",
    "                                                                        unconditional_guidance_scale=scale,\n",
    "                                                                        unconditional_conditioning=sc, \n",
    "                                                                        eta=ddim_eta,\n",
    "                                                                        keep_intermediates=False,\n",
    "                                                                        intermediate_step = steps*2+1,\n",
    "                                                                        steps_per_sampling = 1,\n",
    "                                                                        total_steps = ddim_steps_student)     \n",
    "\n",
    "                                            # decode_teacher = student.decode_first_stage(pred_x0_teacher)\n",
    "                                            # reconstruct_teacher = torch.clamp((decode_teacher+1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        signal = at\n",
    "                                        noise = 1 - at\n",
    "                                        log_snr = torch.log(signal / noise)\n",
    "                                        weight = max(log_snr, 1)\n",
    "                                        loss = weight *  criterion(pred_x0_student, pred_x0_teacher.detach())     \n",
    "                                        # loss = weight * criterion(reconstruct_student, reconstruct_teacher.detach())                    \n",
    "                                        loss.backward()\n",
    "                                        optimizer.step()\n",
    "                                        scheduler.step()\n",
    "                                        # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                        losses.append(loss.item())\n",
    "\n",
    "                                        \n",
    "                        if  generation > 0 and generation % 2 == 0: # or instance==1:\n",
    "\n",
    "                                with torch.no_grad():\n",
    "                                    # the x0 version keeps max denoising steps to 64\n",
    "                                    images, _ = util.compare_teacher_student_x0(original, sampler_original, student, sampler_student, steps=[8], prompt=992, x0=x0)\n",
    "                                    # images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                                    # wandb.log({\"pred_x0\": images})\n",
    "                                    clear_output(wait=True)\n",
    "                                    display(images)\n",
    "                                    # # Optional; compare the images but also change the denoising schedule\n",
    "                                    # images, _ = util.compare_teacher_student(original, sampler_original, student, sampler_student, steps=[64, 16, 8,  4, 2, 1], prompt=992,x0=x0)\n",
    "                                    # # images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                                    # # wandb.log({\"with_sched\": images})\n",
    "\n",
    "                                    # Important: Reset the schedule, as compare_teacher_student changes max steps. \n",
    "                                    sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "                                    sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D-SD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
