{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is used as an actual notebook, for testing Meta's DiT model with distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_distillation_CIN(student, sampler_student, original, sampler_original, optimizer, scheduler,\n",
    "            session=None, steps=20, gradient_updates=200, run_name=\"test\",step_scheduler=\"naive\", x0=False):\n",
    "    \"\"\"\n",
    "    Params: student, sampler_student, original, sampler_original, optimizer, scheduler, session=None, steps=20, generations=200, run_name=\"test\", decrease_steps=False, step_scheduler=\"deterministic\"\n",
    "\n",
    "    Task:Distill a model into itself. This is done by having a (teacher) model distill knowledge into itself. Copies of the original model and sampler \n",
    "    are passed in to compare the original untrained version with the distilled model at scheduled intervals.\n",
    "    \"\"\"\n",
    "    NUM_CLASSES = 1000\n",
    "    ddim_steps_student = steps # Setting the number of steps for the student model\n",
    "    ddim_eta = 0.0 # Setting the eta value to 0.0 means a deterministic output given the original noise, essential\n",
    "    # For both the student and the original model, the number of steps is set to the same value. \n",
    "    # Technically the original model does not need to be trained, but it is kept for comparison purposes.\n",
    "    sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "    sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "    ddim_eta = 0.0 # Setting the eta value to 0.0 means a deterministic output given the original noise, essential\n",
    "    scale = 3.0 # This is $w$ in the paper, the CFG scale. Can be left static or varied as is done occasionally.\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "    instance = 0 # Actual instance of student gradient updates\n",
    "    generation = 0 # The amount of final-step images generated\n",
    "    averaged_losses = []\n",
    "    all_losses = []\n",
    "    \n",
    "    if step_scheduler == \"iterative\": # Halve the number of steps from start to 1 with even allocation of gradient updates\n",
    "        halvings = math.floor(math.log(ddim_steps_student)/math.log(2))\n",
    "        updates_per_halving = int(gradient_updates / halvings)\n",
    "        step_sizes = []\n",
    "        for i in range(halvings):\n",
    "            step_sizes.append(int((steps) / (2**i)))\n",
    "        update_list = []\n",
    "        for i in step_sizes:\n",
    "            update_list.append(int(updates_per_halving / int(i/ 2))) # /2 because of 2 steps per update\n",
    "    elif step_scheduler == \"naive\": # Naive approach, evenly distribute gradient updates over all steps\n",
    "        step_sizes=[ddim_steps_student]\n",
    "        update_list=[gradient_updates // int(ddim_steps_student / 2)] # /2 because of 2 steps per update\n",
    "    elif step_scheduler == \"gradual_linear\": # Gradually decrease the number of steps to 1, with even allocation of gradient updates\n",
    "        step_sizes = np.arange(steps, 0, -2)\n",
    "        update_list = ((1/len(np.append(step_sizes[1:], 1)) * gradient_updates / np.append(step_sizes[1:], 1))).astype(int) * 2 # *2 because of 2 steps per update\n",
    "    elif step_scheduler == \"gradual_exp\": # TEMPORARY VERSION, to test if focus on higher steps is better, reverse of the one below\n",
    "        step_sizes = np.arange(64, 0, -2)\n",
    "        update_list = np.exp((1 / np.append(step_sizes[1:],1))[::-1]) / np.sum(np.exp((1 / np.append(step_sizes[1:],1))[::-1]))\n",
    "        update_list = (update_list * gradient_updates /  np.append(step_sizes[1:],1)).astype(int) * 2 # *2 because of 2 steps per update\n",
    "\n",
    "    with torch.no_grad():\n",
    "        student.use_ema = False\n",
    "        with student.ema_scope(): \n",
    "                if x0:\n",
    "                    sc=None\n",
    "                else:\n",
    "                    sc = student.get_learned_conditioning({student.cond_stage_key: torch.tensor(1*[1000]).to(student.device)}) # Get the learned conditioning\n",
    "                for i, step in enumerate(step_sizes): # For each step size\n",
    "                    if instance != 0 and \"gradual\" not in step_scheduler:   # Save the model after every step size. Given the large model size, \n",
    "                                                                            # the gradual versions are not saved each time (steps * 2 * 4.7gb is a lot!)\n",
    "                        util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "                    updates = int(step / 2) # We take updates as half the step size, because we do 2 steps per update\n",
    "                    generations = update_list[i] # The number of generations has been determined earlier\n",
    "                    print(\"Distilling to:\", step)\n",
    "                    \n",
    "                    with tqdm.tqdm(torch.randint(0, NUM_CLASSES, (generations,))) as tepoch: # Take a random class for each generation\n",
    "\n",
    "                        for i, class_prompt in enumerate(tepoch):\n",
    "                            generation += 1\n",
    "                            losses = []       \n",
    "                            \n",
    "                            scale = np.random.uniform(1.0, 4.0) # Randomly sample a scale for each generation, optional\n",
    "                            c_student = student.get_learned_conditioning({student.cond_stage_key: torch.tensor([class_prompt]).to(student.device)}) # Set to 0 for unconditional, requires pretraining\n",
    "                            \n",
    "                            samples_ddim= None # Setting to None will create a new noise vector for each generation\n",
    "                            predictions_temp = []\n",
    "                            \n",
    "                            for steps in range(updates):\n",
    "                                # with autocast() and torch.enable_grad(): # For mixed precision training, should not be used for final results\n",
    "                                    with torch.enable_grad():\n",
    "                                            instance += 1\n",
    "                                            \n",
    "                                            optimizer.zero_grad()\n",
    "                                            samples_ddim, pred_x0_student, _, at= sampler_student.sample_student(S=1,\n",
    "                                                                                conditioning=c_student,\n",
    "                                                                                batch_size=1,\n",
    "                                                                                shape=[3, 64, 64],\n",
    "                                                                                verbose=False,\n",
    "                                                                                x_T=samples_ddim, # start noise or teacher output\n",
    "                                                                                unconditional_guidance_scale=scale,\n",
    "                                                                                unconditional_conditioning=sc, \n",
    "                                                                                eta=ddim_eta,\n",
    "                                                                                keep_intermediates=False,\n",
    "                                                                                intermediate_step = steps*2,\n",
    "                                                                                steps_per_sampling = 1,\n",
    "                                                                                total_steps = ddim_steps_student)\n",
    "                                            \n",
    "                                            # Code below first decodes the latent image and then reconstructs it. This is not necessary, but can be used to check if the latent image is correct\n",
    "                                            # decode_student = student.differentiable_decode_first_stage(pred_x0_student)\n",
    "                                            # reconstruct_student = torch.clamp((decode_student+1.0)/2.0, min=0.0, max=1.0)\n",
    "                               \n",
    "\n",
    "                                            with torch.no_grad():\n",
    "                                                samples_ddim.detach()\n",
    "                                                samples_ddim, _, _, pred_x0_teacher, _ = sampler_student.sample(S=1,\n",
    "                                                                            conditioning=c_student,\n",
    "                                                                            batch_size=1,\n",
    "                                                                            shape=[3, 64, 64],\n",
    "                                                                            verbose=False,\n",
    "                                                                            x_T=samples_ddim, # output of student\n",
    "                                                                            unconditional_guidance_scale=scale,\n",
    "                                                                            unconditional_conditioning=sc, \n",
    "                                                                            eta=ddim_eta,\n",
    "                                                                            keep_intermediates=False,\n",
    "                                                                            intermediate_step = steps*2+1,\n",
    "                                                                            steps_per_sampling = 1,\n",
    "                                                                            total_steps = ddim_steps_student)     \n",
    "\n",
    "                                                # decode_teacher = student.decode_first_stage(pred_x0_teacher)\n",
    "                                                # reconstruct_teacher = torch.clamp((decode_teacher+1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        \n",
    "                                            \n",
    "                                            # # NO AUTOCAST:\n",
    "                                            signal = at\n",
    "                                            noise = 1 - at\n",
    "                                            log_snr = torch.log(signal / noise)\n",
    "                                            weight = max(log_snr, 1)\n",
    "                                            loss = weight * criterion(pred_x0_student, pred_x0_teacher.detach())                     \n",
    "                                            loss.backward()\n",
    "                                            optimizer.step()\n",
    "                                            # scheduler.step()\n",
    "                                            # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                            losses.append(loss.item())\n",
    "\n",
    "\n",
    "                                            if session != None and instance % 400 == 0: # or instance==1:\n",
    "\n",
    "                                                with torch.no_grad():\n",
    "                                                    # the x0 version keeps max denoising steps to 64\n",
    "                                                    images, _ = util.compare_teacher_student_x0(original, sampler_original, student, sampler_student, steps=[16, 8,  4, 2, 1], prompt=992, x0=x0)\n",
    "                                                    images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                                                    wandb.log({\"pred_x0\": images})\n",
    "\n",
    "                                                    # Important: Reset the schedule, as compare_teacher_student changes max steps. \n",
    "                                                    sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "                                                    sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "                            all_losses.extend(losses)\n",
    "                            averaged_losses.append(sum(losses) / len(losses))\n",
    "                            if session != None:\n",
    "                                session.log({\"generation_loss\":averaged_losses[-1]})\n",
    "                            tepoch.set_postfix(epoch_loss=averaged_losses[-1])\n",
    "\n",
    "                if step_scheduler == \"naive\" or \"gradual\" in step_scheduler: # Save the final model, since we skipped all the intermediate steps\n",
    "                    util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers timm --upgrade\n",
    "!pip install filelock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_DiT import *\n",
    "import os\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from download import find_model\n",
    "# from models import DiT_XL_2\n",
    "from models import DiT_S_2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "# torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up image sizes\n",
    "image_size = 256 #@param [256, 512]\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
    "latent_size = int(image_size) // 8\n",
    "\n",
    "# Load model:\n",
    "model = DiT_XL_2(input_size=latent_size).to(device)\n",
    "# model = DiT_XL_2(input_size=latent_size).to(device)\n",
    "# model = DiT_S_2(input_size=latent_size).to(device)\n",
    "\n",
    "state_dict = find_model(f\"DiT-XL-2-{image_size}x{image_size}.pt\")\n",
    "# state_dict = find_model(f\"DiT-S-2-{image_size}x{image_size}.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval() # important!\n",
    "vae = AutoencoderKL.from_pretrained(vae_model).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing a single denoising step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   </span>sample_fn, z.shape, z, clip_denoised=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>, model_kwargs=model_kwargs, progress=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">Fals</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>27 samples, _ = samples.chunk(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>, dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Remove null class samples</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span>samples = vae.decode(samples / <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.18215</span>).sample                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span>save_image(samples, <span style=\"color: #808000; text-decoration-color: #808000\">\"sample.png\"</span>, nrow=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(samples_per_row),                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'generator'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'chunk'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m27\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   \u001b[0msample_fn, z.shape, z, clip_denoised=\u001b[94mFalse\u001b[0m, model_kwargs=model_kwargs, progress=\u001b[94mFals\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m)                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m27 samples, _ = samples.chunk(\u001b[94m2\u001b[0m, dim=\u001b[94m0\u001b[0m)  \u001b[2m# Remove null class samples\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0msamples = vae.decode(samples / \u001b[94m0.18215\u001b[0m).sample                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0msave_image(samples, \u001b[33m\"\u001b[0m\u001b[33msample.png\u001b[0m\u001b[33m\"\u001b[0m, nrow=\u001b[96mint\u001b[0m(samples_per_row),                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'generator'\u001b[0m object has no attribute \u001b[32m'chunk'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 1\n",
    "num_sampling_steps = 4 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = [207]\n",
    "samples_per_row = 4 #@param {type:\"number\"}\n",
    "\n",
    "# Create diffusion object:\n",
    "diffusion = create_diffusion(str(num_sampling_steps))\n",
    "# Sample inputs:\n",
    "z = torch.randn(1, model.in_channels, latent_size, latent_size, device=device)\n",
    "y = torch.randint(0, 1, (n,), device=device)\n",
    "\n",
    "# Setup classifier-free guidance:\n",
    "\n",
    "z = torch.cat([z, z], 0)\n",
    "y_null = torch.tensor([1000] * n, device=device)\n",
    "y = torch.cat([y, y_null], 0)\n",
    "model_kwargs = dict(y=y, cfg_scale=4)\n",
    "sample_fn = model.forward_with_cfg\n",
    "\n",
    "\n",
    "# Sample images:\n",
    "samples = diffusion.ddim_sample_loop_progressive(\n",
    "    sample_fn, z.shape, z, clip_denoised=False, model_kwargs=model_kwargs, progress=False, device=device\n",
    ")\n",
    "\n",
    "samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "\n",
    "samples = vae.decode(samples / 0.18215).sample\n",
    "save_image(samples, \"sample.png\", nrow=int(samples_per_row), \n",
    "           normalize=True, value_range=(-1, 1))\n",
    "samples = Image.open(\"sample.png\")\n",
    "display(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 samples, pred_xstart = sample_step_grad(model.forward_with_cfg_grad, diffusion, step, mo     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'sample_step_grad'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 samples, pred_xstart = sample_step_grad(model.forward_with_cfg_grad, diffusion, step, mo     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'sample_step_grad'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples, pred_xstart = sample_step_grad(model.forward_with_cfg_grad, diffusion, 4, model_kwargs, timesteps, samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "from util_DiT import *\n",
    "import os\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from download import find_model\n",
    "from models import DiT_XL_2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "torch.set_grad_enabled(True)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import tqdm\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "device = \"cuda\"\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# Setting up image sizes\n",
    "image_size = 256 #@param [256, 512]\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
    "latent_size = int(image_size) // 8\n",
    "\n",
    "with autocast():\n",
    "    # Load model:\n",
    "    model = DiT_XL_2(input_size=latent_size).to(device)\n",
    "    # original = DiT_XL_2(input_size=latent_size).to(device)\n",
    "    state_dict = find_model(f\"DiT-XL-2-{image_size}x{image_size}.pt\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    # original.load_state_dict(state_dict)\n",
    "    # original.eval()\n",
    "    model.eval() # important!\n",
    "    vae = AutoencoderKL.from_pretrained(vae_model).to(\"cpu\")\n",
    "del state_dict, vae_model\n",
    "torch.cuda.empty_cache()\n",
    "steps = 20\n",
    "generations = 10\n",
    "decrease_steps = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embed\n",
      "x_embedder.proj.weight\n",
      "x_embedder.proj.bias\n",
      "t_embedder.mlp.0.weight\n",
      "t_embedder.mlp.0.bias\n",
      "t_embedder.mlp.2.weight\n",
      "t_embedder.mlp.2.bias\n",
      "y_embedder.embedding_table.weight\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.0.adaLN_modulation.1.weight\n",
      "blocks.0.adaLN_modulation.1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.1.adaLN_modulation.1.weight\n",
      "blocks.1.adaLN_modulation.1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.2.adaLN_modulation.1.weight\n",
      "blocks.2.adaLN_modulation.1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.3.adaLN_modulation.1.weight\n",
      "blocks.3.adaLN_modulation.1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.4.adaLN_modulation.1.weight\n",
      "blocks.4.adaLN_modulation.1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.5.adaLN_modulation.1.weight\n",
      "blocks.5.adaLN_modulation.1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.6.adaLN_modulation.1.weight\n",
      "blocks.6.adaLN_modulation.1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.7.adaLN_modulation.1.weight\n",
      "blocks.7.adaLN_modulation.1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.8.adaLN_modulation.1.weight\n",
      "blocks.8.adaLN_modulation.1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.9.adaLN_modulation.1.weight\n",
      "blocks.9.adaLN_modulation.1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.10.adaLN_modulation.1.weight\n",
      "blocks.10.adaLN_modulation.1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "blocks.11.adaLN_modulation.1.weight\n",
      "blocks.11.adaLN_modulation.1.bias\n",
      "blocks.12.attn.qkv.weight\n",
      "blocks.12.attn.qkv.bias\n",
      "blocks.12.attn.proj.weight\n",
      "blocks.12.attn.proj.bias\n",
      "blocks.12.mlp.fc1.weight\n",
      "blocks.12.mlp.fc1.bias\n",
      "blocks.12.mlp.fc2.weight\n",
      "blocks.12.mlp.fc2.bias\n",
      "blocks.12.adaLN_modulation.1.weight\n",
      "blocks.12.adaLN_modulation.1.bias\n",
      "blocks.13.attn.qkv.weight\n",
      "blocks.13.attn.qkv.bias\n",
      "blocks.13.attn.proj.weight\n",
      "blocks.13.attn.proj.bias\n",
      "blocks.13.mlp.fc1.weight\n",
      "blocks.13.mlp.fc1.bias\n",
      "blocks.13.mlp.fc2.weight\n",
      "blocks.13.mlp.fc2.bias\n",
      "blocks.13.adaLN_modulation.1.weight\n",
      "blocks.13.adaLN_modulation.1.bias\n",
      "blocks.14.attn.qkv.weight\n",
      "blocks.14.attn.qkv.bias\n",
      "blocks.14.attn.proj.weight\n",
      "blocks.14.attn.proj.bias\n",
      "blocks.14.mlp.fc1.weight\n",
      "blocks.14.mlp.fc1.bias\n",
      "blocks.14.mlp.fc2.weight\n",
      "blocks.14.mlp.fc2.bias\n",
      "blocks.14.adaLN_modulation.1.weight\n",
      "blocks.14.adaLN_modulation.1.bias\n",
      "blocks.15.attn.qkv.weight\n",
      "blocks.15.attn.qkv.bias\n",
      "blocks.15.attn.proj.weight\n",
      "blocks.15.attn.proj.bias\n",
      "blocks.15.mlp.fc1.weight\n",
      "blocks.15.mlp.fc1.bias\n",
      "blocks.15.mlp.fc2.weight\n",
      "blocks.15.mlp.fc2.bias\n",
      "blocks.15.adaLN_modulation.1.weight\n",
      "blocks.15.adaLN_modulation.1.bias\n",
      "blocks.16.attn.qkv.weight\n",
      "blocks.16.attn.qkv.bias\n",
      "blocks.16.attn.proj.weight\n",
      "blocks.16.attn.proj.bias\n",
      "blocks.16.mlp.fc1.weight\n",
      "blocks.16.mlp.fc1.bias\n",
      "blocks.16.mlp.fc2.weight\n",
      "blocks.16.mlp.fc2.bias\n",
      "blocks.16.adaLN_modulation.1.weight\n",
      "blocks.16.adaLN_modulation.1.bias\n",
      "blocks.17.attn.qkv.weight\n",
      "blocks.17.attn.qkv.bias\n",
      "blocks.17.attn.proj.weight\n",
      "blocks.17.attn.proj.bias\n",
      "blocks.17.mlp.fc1.weight\n",
      "blocks.17.mlp.fc1.bias\n",
      "blocks.17.mlp.fc2.weight\n",
      "blocks.17.mlp.fc2.bias\n",
      "blocks.17.adaLN_modulation.1.weight\n",
      "blocks.17.adaLN_modulation.1.bias\n",
      "blocks.18.attn.qkv.weight\n",
      "blocks.18.attn.qkv.bias\n",
      "blocks.18.attn.proj.weight\n",
      "blocks.18.attn.proj.bias\n",
      "blocks.18.mlp.fc1.weight\n",
      "blocks.18.mlp.fc1.bias\n",
      "blocks.18.mlp.fc2.weight\n",
      "blocks.18.mlp.fc2.bias\n",
      "blocks.18.adaLN_modulation.1.weight\n",
      "blocks.18.adaLN_modulation.1.bias\n",
      "blocks.19.attn.qkv.weight\n",
      "blocks.19.attn.qkv.bias\n",
      "blocks.19.attn.proj.weight\n",
      "blocks.19.attn.proj.bias\n",
      "blocks.19.mlp.fc1.weight\n",
      "blocks.19.mlp.fc1.bias\n",
      "blocks.19.mlp.fc2.weight\n",
      "blocks.19.mlp.fc2.bias\n",
      "blocks.19.adaLN_modulation.1.weight\n",
      "blocks.19.adaLN_modulation.1.bias\n",
      "blocks.20.attn.qkv.weight\n",
      "blocks.20.attn.qkv.bias\n",
      "blocks.20.attn.proj.weight\n",
      "blocks.20.attn.proj.bias\n",
      "blocks.20.mlp.fc1.weight\n",
      "blocks.20.mlp.fc1.bias\n",
      "blocks.20.mlp.fc2.weight\n",
      "blocks.20.mlp.fc2.bias\n",
      "blocks.20.adaLN_modulation.1.weight\n",
      "blocks.20.adaLN_modulation.1.bias\n",
      "blocks.21.attn.qkv.weight\n",
      "blocks.21.attn.qkv.bias\n",
      "blocks.21.attn.proj.weight\n",
      "blocks.21.attn.proj.bias\n",
      "blocks.21.mlp.fc1.weight\n",
      "blocks.21.mlp.fc1.bias\n",
      "blocks.21.mlp.fc2.weight\n",
      "blocks.21.mlp.fc2.bias\n",
      "blocks.21.adaLN_modulation.1.weight\n",
      "blocks.21.adaLN_modulation.1.bias\n",
      "blocks.22.attn.qkv.weight\n",
      "blocks.22.attn.qkv.bias\n",
      "blocks.22.attn.proj.weight\n",
      "blocks.22.attn.proj.bias\n",
      "blocks.22.mlp.fc1.weight\n",
      "blocks.22.mlp.fc1.bias\n",
      "blocks.22.mlp.fc2.weight\n",
      "blocks.22.mlp.fc2.bias\n",
      "blocks.22.adaLN_modulation.1.weight\n",
      "blocks.22.adaLN_modulation.1.bias\n",
      "blocks.23.attn.qkv.weight\n",
      "blocks.23.attn.qkv.bias\n",
      "blocks.23.attn.proj.weight\n",
      "blocks.23.attn.proj.bias\n",
      "blocks.23.mlp.fc1.weight\n",
      "blocks.23.mlp.fc1.bias\n",
      "blocks.23.mlp.fc2.weight\n",
      "blocks.23.mlp.fc2.bias\n",
      "blocks.23.adaLN_modulation.1.weight\n",
      "blocks.23.adaLN_modulation.1.bias\n",
      "blocks.24.attn.qkv.weight\n",
      "blocks.24.attn.qkv.bias\n",
      "blocks.24.attn.proj.weight\n",
      "blocks.24.attn.proj.bias\n",
      "blocks.24.mlp.fc1.weight\n",
      "blocks.24.mlp.fc1.bias\n",
      "blocks.24.mlp.fc2.weight\n",
      "blocks.24.mlp.fc2.bias\n",
      "blocks.24.adaLN_modulation.1.weight\n",
      "blocks.24.adaLN_modulation.1.bias\n",
      "blocks.25.attn.qkv.weight\n",
      "blocks.25.attn.qkv.bias\n",
      "blocks.25.attn.proj.weight\n",
      "blocks.25.attn.proj.bias\n",
      "blocks.25.mlp.fc1.weight\n",
      "blocks.25.mlp.fc1.bias\n",
      "blocks.25.mlp.fc2.weight\n",
      "blocks.25.mlp.fc2.bias\n",
      "blocks.25.adaLN_modulation.1.weight\n",
      "blocks.25.adaLN_modulation.1.bias\n",
      "blocks.26.attn.qkv.weight\n",
      "blocks.26.attn.qkv.bias\n",
      "blocks.26.attn.proj.weight\n",
      "blocks.26.attn.proj.bias\n",
      "blocks.26.mlp.fc1.weight\n",
      "blocks.26.mlp.fc1.bias\n",
      "blocks.26.mlp.fc2.weight\n",
      "blocks.26.mlp.fc2.bias\n",
      "blocks.26.adaLN_modulation.1.weight\n",
      "blocks.26.adaLN_modulation.1.bias\n",
      "blocks.27.attn.qkv.weight\n",
      "blocks.27.attn.qkv.bias\n",
      "blocks.27.attn.proj.weight\n",
      "blocks.27.attn.proj.bias\n",
      "blocks.27.mlp.fc1.weight\n",
      "blocks.27.mlp.fc1.bias\n",
      "blocks.27.mlp.fc2.weight\n",
      "blocks.27.mlp.fc2.bias\n",
      "blocks.27.adaLN_modulation.1.weight\n",
      "blocks.27.adaLN_modulation.1.bias\n",
      "final_layer.linear.weight\n",
      "final_layer.linear.bias\n",
      "final_layer.adaLN_modulation.1.weight\n",
      "final_layer.adaLN_modulation.1.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking to see whether updating is even possible, it is, but only with the linear layers as they dont take up much memory\n",
    "\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'linear' in name:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': params_to_update, 'lr': 0.001},  # Parameters to update\n",
    "   # Other parameters (not updated)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Set user inputs:\n",
    "# # seed = 0 #@param {type:\"number\"}\n",
    "# # torch.manual_seed(seed)\n",
    "# num_sampling_steps = 4 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "# cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "# class_labels = [992] #@param {type:\"raw\"}\n",
    "# samples_per_row = 1 #@param {type:\"number\"}\n",
    "\n",
    "# # Create diffusion object:\n",
    "# diffusion = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "# # Create sampling noise:\n",
    "# n = 1\n",
    "# z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "# y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "# # Setup classifier-free guidance:\n",
    "# z = torch.cat([z, z], 0)\n",
    "# y_null = torch.tensor([1000] * n, device=device)\n",
    "# y = torch.cat([y, y_null], 0)\n",
    "# model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "# # Sample images:\n",
    "# samples = diffusion.ddim_sample_loop_progressive(\n",
    "#     model.forward_with_cfg, z.shape, z, clip_denoised=False, \n",
    "#     model_kwargs=model_kwargs, progress=True, device=device\n",
    "# )\n",
    "# samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "# samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# # Save and display images:\n",
    "# save_image(samples, \"sample.png\", nrow=int(samples_per_row), \n",
    "#            normalize=True, value_range=(-1, 1))\n",
    "# samples = Image.open(\"sample.png\")\n",
    "# display(samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With intermediate steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set user inputs:\n",
    "# seed = 0 #@param {type:\"number\"}\n",
    "# torch.manual_seed(seed)\n",
    "# import tqdm\n",
    "# num_sampling_steps =2 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "# cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "# class_labels = [992] #@param {type:\"raw\"}\n",
    "# samples_per_row = 1 #@param {type:\"number\"}\n",
    "\n",
    "# # Create diffusion object:\n",
    "# diffusion = create_diffusion(str(num_sampling_steps))\n",
    "# diffusion_original = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "# # Create sampling noise:\n",
    "# n = 1\n",
    "# z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "# y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "# # Setup classifier-free guidance:\n",
    "# z = torch.cat([z, z], 0)\n",
    "# y_null = torch.tensor([1000] * n, device=device)\n",
    "# y = torch.cat([y, y_null], 0)\n",
    "# model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "\n",
    "# samples = torch.randn(z.shape, device=device)\n",
    "# indices = list(range(diffusion.num_timesteps))[::-1]\n",
    "# for i in tqdm.tqdm(indices):\n",
    "#     print(i)\n",
    "#     samples = diffusion.ddim_sample_loop_progressive_intermediate(\n",
    "#             model.forward_with_cfg, z.shape, noise=samples, clip_denoised=False, \n",
    "#             model_kwargs=model_kwargs, progress=False, device=device, step=i)\n",
    "#     img, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "#     img = vae.decode(img / 0.18215).sample\n",
    "#     if i == 1:\n",
    "#         last = img\n",
    "#     else:\n",
    "#         final = img\n",
    "#     # Save and display images:\n",
    "#     save_image(img, \"sample.png\", nrow=int(samples_per_row), \n",
    "#             normalize=True, value_range=(-1, 1))\n",
    "#     img = Image.open(\"sample.png\")\n",
    "#     display(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling to: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]C:\\Users\\Damion\\AppData\\Local\\Temp\\ipykernel_21532\\2013161711.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(class_labels, device=device)\n",
      "  5%|▌         | 1/20 [00:06<02:05,  6.63s/it, epoch_loss=0.000634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00063366120448336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:10<01:32,  5.11s/it, epoch_loss=0.000708]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007082553289365024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:14<01:18,  4.61s/it, epoch_loss=0.000913]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009127709694439545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:18<01:09,  4.37s/it, epoch_loss=0.00118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001177606068085879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:22<01:03,  4.25s/it, epoch_loss=0.00136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00136482025263831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:26<00:58,  4.15s/it, epoch_loss=0.00143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014326108328532427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:30<00:57,  4.39s/it, epoch_loss=0.00165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016477228025905788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">222</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">219 # optimizer</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220 # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221 # self_distillation_dit(model, original, optimizer, step_scheduler=\"naive\")</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>222 self_distillation_dit(model, optimizer, step_scheduler=<span style=\"color: #808000; text-decoration-color: #808000\">\"naive\"</span>)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">223 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">self_distillation_dit</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">96</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   │   │   │   │   │   │   │   │   │   │   </span>instance += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 │   │   │   │   │   │   │   │   │   │   │   </span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 │   │   │   │   │   │   │   │   │   │   │   </span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 96 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │   </span>samples_student = diffusion.ddim_sample_loop   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 │   │   │   │   │   │   │   │   │   │   │   │   │   </span>model.forward_with_cfg, z.shape, noi   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 │   │   │   │   │   │   │   │   │   │   │   │   │   </span>model_kwargs=model_kwargs, progress=   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\diffusion\\gaussian_diffusion.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">896</span> in                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ddim_sample_loop_progressive_intermediate</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">893 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">894 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> student:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">895 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> th.enable_grad():                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>896 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>out = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ddim_sample(                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">897 │   │   │   │   │   </span>model,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">898 │   │   │   │   │   </span>img,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">899 │   │   │   │   │   </span>t,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\diffusion\\gaussian_diffusion.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">528</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ddim_sample</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">525 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Sample x_{t-1} from the model using DDIM.</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">526 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Same usage as p_sample().</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">527 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>528 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>out = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.p_mean_variance(                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">529 │   │   │   </span>model,                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">530 │   │   │   </span>x,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">531 │   │   │   </span>t,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\diffusion\\respace.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">92</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">p_mean_variance</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 89 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">p_mean_variance</span>(                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, model, *args, **kwargs                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 │   </span>):  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># pylint: disable=signature-differs</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 92 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().p_mean_variance(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._wrap_model(model), *args, **kwargs)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_losses</span>(                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, model, *args, **kwargs                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\diffusion\\gaussian_diffusion.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">279</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">p_mean_variance</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">276 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 │   │   </span>B, C = x.shape[:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>]                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> t.shape == (B,)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>279 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model_output = model(x, t, **model_kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">280 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(model_output, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 │   │   │   </span>model_output, extra = model_output                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\diffusion\\respace.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">129</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   </span>new_ts = map_tensor[ts]                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 │   │   # if self.rescale_timesteps:</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">128 │   │   #     new_ts = new_ts.float() * (1000.0 / self.original_num_steps)</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>129 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(x, new_ts, **kwargs)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">130 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\models.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">260</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward_with_cfg</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">257 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">258 │   │   </span>half = x[: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(x) // <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>]                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">259 │   │   </span>combined = torch.cat([half, half], dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>260 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model_out = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.forward(combined, timesteps, y)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">261 │   │   # For exact reproducibility reasons, we apply classifier-free guidance on only</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">262 │   │   # three channels by default. The standard approach to cfg applies it to all chan</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">263 │   │   # This can be done by uncommenting the following line and commenting-out the lin</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\models.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">247</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">244 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">245 │   │   </span>c = t + y                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (N, D)</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">246 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> block <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.blocks:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>247 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = block(x, c)                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (N, T, D)</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">248 │   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.final_layer(x, c)                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (N, T, patch_size ** 2 * out_channel</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">249 │   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.unpatchify(x)                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (N, out_channels, H, W)</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">250 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\models.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">121</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x, c):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 │   │   </span>shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.adaLN_modu   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 │   │   </span>x = x + gate_msa.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>) * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(modulate(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm1(x), shift_msa, sca   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>121 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x + gate_mlp.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>) * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mlp(modulate(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm2(x), shift_mlp, scal   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Diffusion_Thesis\\cin_256\\models.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">modulate</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 18 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">modulate</span>(x, shift, scale):                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 20 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x * (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> + scale.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)) + shift.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 23 #################################################################################</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m222\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m# optimizer\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m# self_distillation_dit(model, original, optimizer, step_scheduler=\"naive\")\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m222 self_distillation_dit(model, optimizer, step_scheduler=\u001b[33m\"\u001b[0m\u001b[33mnaive\u001b[0m\u001b[33m\"\u001b[0m)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m223 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mself_distillation_dit\u001b[0m:\u001b[94m96\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   \u001b[0minstance += \u001b[94m1\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   \u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   \u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 96 \u001b[2m│   │   │   │   │   │   │   │   │   │   │   \u001b[0msamples_student = diffusion.ddim_sample_loop   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mmodel.forward_with_cfg, z.shape, noi   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mmodel_kwargs=model_kwargs, progress=   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\diffusion\\gaussian_diffusion.py\u001b[0m:\u001b[94m896\u001b[0m in                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mddim_sample_loop_progressive_intermediate\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m893 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m894 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m student:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m895 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m th.enable_grad():                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m896 \u001b[2m│   │   │   │   \u001b[0mout = \u001b[96mself\u001b[0m.ddim_sample(                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m897 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmodel,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m898 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mimg,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m899 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mt,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\diffusion\\gaussian_diffusion.py\u001b[0m:\u001b[94m528\u001b[0m in \u001b[92mddim_sample\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m525 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mSample x_{t-1} from the model using DDIM.\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m526 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mSame usage as p_sample().\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m527 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m528 \u001b[2m│   │   \u001b[0mout = \u001b[96mself\u001b[0m.p_mean_variance(                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m529 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel,                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m530 \u001b[0m\u001b[2m│   │   │   \u001b[0mx,                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m531 \u001b[0m\u001b[2m│   │   │   \u001b[0mt,                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\diffusion\\respace.py\u001b[0m:\u001b[94m92\u001b[0m in \u001b[92mp_mean_variance\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mp_mean_variance\u001b[0m(                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m, model, *args, **kwargs                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   \u001b[0m):  \u001b[2m# pylint: disable=signature-differs\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 92 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().p_mean_variance(\u001b[96mself\u001b[0m._wrap_model(model), *args, **kwargs)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mtraining_losses\u001b[0m(                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m, model, *args, **kwargs                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\diffusion\\gaussian_diffusion.py\u001b[0m:\u001b[94m279\u001b[0m in \u001b[92mp_mean_variance\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m276 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   │   \u001b[0mB, C = x.shape[:\u001b[94m2\u001b[0m]                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m t.shape == (B,)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m279 \u001b[2m│   │   \u001b[0mmodel_output = model(x, t, **model_kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m280 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(model_output, \u001b[96mtuple\u001b[0m):                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_output, extra = model_output                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\diffusion\\respace.py\u001b[0m:\u001b[94m129\u001b[0m in \u001b[92m__call__\u001b[0m                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0mnew_ts = map_tensor[ts]                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# if self.rescale_timesteps:\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#     new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m129 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.model(x, new_ts, **kwargs)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m130 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\models.py\u001b[0m:\u001b[94m260\u001b[0m in \u001b[92mforward_with_cfg\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m257 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m258 \u001b[0m\u001b[2m│   │   \u001b[0mhalf = x[: \u001b[96mlen\u001b[0m(x) // \u001b[94m2\u001b[0m]                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m259 \u001b[0m\u001b[2m│   │   \u001b[0mcombined = torch.cat([half, half], dim=\u001b[94m0\u001b[0m)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m260 \u001b[2m│   │   \u001b[0mmodel_out = \u001b[96mself\u001b[0m.forward(combined, timesteps, y)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# For exact reproducibility reasons, we apply classifier-free guidance on only\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# three channels by default. The standard approach to cfg applies it to all chan\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m263 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# This can be done by uncommenting the following line and commenting-out the lin\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\models.py\u001b[0m:\u001b[94m247\u001b[0m in \u001b[92mforward\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m244 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m245 \u001b[0m\u001b[2m│   │   \u001b[0mc = t + y                                \u001b[2m# (N, D)\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m block \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.blocks:                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m247 \u001b[2m│   │   │   \u001b[0mx = block(x, c)                      \u001b[2m# (N, T, D)\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m248 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.final_layer(x, c)                \u001b[2m# (N, T, patch_size ** 2 * out_channel\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.unpatchify(x)                   \u001b[2m# (N, out_channels, H, W)\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m250 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m x                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\models.py\u001b[0m:\u001b[94m121\u001b[0m in \u001b[92mforward\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x, c):                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   │   \u001b[0mshift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = \u001b[96mself\u001b[0m.adaLN_modu   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   \u001b[0mx = x + gate_msa.unsqueeze(\u001b[94m1\u001b[0m) * \u001b[96mself\u001b[0m.attn(modulate(\u001b[96mself\u001b[0m.norm1(x), shift_msa, sca   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m121 \u001b[2m│   │   \u001b[0mx = x + gate_mlp.unsqueeze(\u001b[94m1\u001b[0m) * \u001b[96mself\u001b[0m.mlp(modulate(\u001b[96mself\u001b[0m.norm2(x), shift_mlp, scal   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m x                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Diffusion_Thesis\\cin_256\\models.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmodulate\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 18 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 19 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mmodulate\u001b[0m(x, shift, scale):                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 20 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m x * (\u001b[94m1\u001b[0m + scale.unsqueeze(\u001b[94m1\u001b[0m)) + shift.unsqueeze(\u001b[94m1\u001b[0m)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 21 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 22 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 23 \u001b[0m\u001b[2m#################################################################################\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def self_distillation_dit(model, optimizer,\n",
    "            session=None, steps=20, generations=200, early_stop=True, run_name=\"test\", decrease_steps=False,\n",
    "            step_scheduler=\"deterministic\", type=\"snellius\"):\n",
    "    \"\"\"\n",
    "    Distill a model into itself. This is done by having a (teacher) model distill knowledge into itself. Copies of the original model and sampler \n",
    "    are passed in to compare the original untrained version with the distilled model at scheduled intervals.\n",
    "    \"\"\"\n",
    "    NUM_CLASSES = 1000\n",
    "    gradient_updates = generations\n",
    "    ddim_steps_student = steps\n",
    "    TEACHER_STEPS = 2\n",
    "    ddim_eta = 0.0\n",
    "    scale = 3.0\n",
    "    optimizer=optimizer\n",
    "    averaged_losses = []\n",
    "    criterion = nn.MSELoss()\n",
    "    instance = 0\n",
    "    generation = 0\n",
    "    all_losses = []\n",
    "    num_sampling_steps = 64\n",
    "\n",
    "    if step_scheduler == \"iterative\":\n",
    "        halvings = math.floor(math.log(64)/math.log(2))\n",
    "        updates_per_halving = int(gradient_updates / halvings)\n",
    "        step_sizes = []\n",
    "        for i in range(halvings):\n",
    "            step_sizes.append(int((steps) / (2**i)))\n",
    "        update_list = []\n",
    "        for i in step_sizes:\n",
    "            update_list.append(int(updates_per_halving / int(i/ 2)))\n",
    "    elif step_scheduler == \"naive\":\n",
    "        step_sizes=[ddim_steps_student]\n",
    "        update_list=[gradient_updates // int(ddim_steps_student / 2)]\n",
    "    elif step_scheduler == \"gradual_linear\":\n",
    "        step_sizes = np.arange(steps, 0, -2)\n",
    "        update_list = (1/len(np.append(step_sizes[1:], 1)) * gradient_updates / np.append(step_sizes[1:], 1)).astype(int)\n",
    "    elif step_scheduler == \"gradual_exp\":\n",
    "        step_sizes = np.arange(64, 0, -2)\n",
    "        update_list = np.exp(1 / np.append(step_sizes[1:],1)) / np.sum(np.exp(1 / np.append(step_sizes[1:],1)))\n",
    "        update_list = (update_list * gradient_updates /  np.append(step_sizes[1:],1)).astype(int)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    diffusion = create_diffusion(str(num_sampling_steps))\n",
    "    \n",
    "    n = 1\n",
    "    num_sampling_steps =4 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "    cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "    samples_per_row = 1 #@param {type:\"number\"}\n",
    "    indices = list(range(diffusion.num_timesteps))[::-1]\n",
    "    with torch.no_grad():\n",
    "        # with student.ema_scope():              \n",
    "              \n",
    "\n",
    "                for i, step in enumerate(step_sizes):\n",
    "                    # if instance != 0 and \"gradual\" not in step_scheduler:\n",
    "                    #     util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "                    updates = int(step / 2)\n",
    "                    generations = update_list[i]\n",
    "                    print(\"Distilling to:\", updates)\n",
    "                         \n",
    "                    \n",
    "                    # sc = student.get_learned_conditioning({student.cond_stage_key: torch.tensor(1*[1000]).to(student.device)})\n",
    "                    \n",
    "                    \n",
    "                    with tqdm.tqdm(torch.randint(0, NUM_CLASSES, (generations,))) as tepoch:\n",
    "\n",
    "                        for i, class_prompt in enumerate(tepoch):\n",
    "                            generation += 1\n",
    "                            losses = []        \n",
    "                            class_labels = torch.tensor([class_prompt])\n",
    "                            z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "                            y = torch.tensor(class_labels, device=device)\n",
    "                            z = torch.cat([z, z], 0)\n",
    "                            y_null = torch.tensor([1000] * n, device=device)\n",
    "                            y = torch.cat([y, y_null], 0)\n",
    "                            model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "                            # teacher_kwargs = model_kwargs.copy()\n",
    "                            samples_ddim= None\n",
    "                            predictions_temp = []\n",
    "                            \n",
    "                            samples_teacher = torch.randn(z.shape, device=device)\n",
    "                                \n",
    "                            for steps in range(updates):  \n",
    "                                    optimizer.zero_grad()\n",
    "                                    with autocast():\n",
    "\n",
    "                                        with torch.enable_grad():\n",
    "                                            \n",
    "                                            instance += 1\n",
    "                                        \n",
    "                                        \n",
    "                                            samples_student = diffusion.ddim_sample_loop_progressive_intermediate(\n",
    "                                                    model.forward_with_cfg, z.shape, noise=samples_teacher, clip_denoised=False, \n",
    "                                                    model_kwargs=model_kwargs, progress=False, device=device, step=steps*2, student=True)\n",
    "                                            \n",
    "                                            samples_student_x0, _ = samples_student.chunk(2, dim=0)\n",
    "                                            # samples_student_x0, _ = samples.chunk(2, dim=0)\n",
    "                                            samples_student.detach()\n",
    "\n",
    "                                        with torch.no_grad():\n",
    "                                            \n",
    "                                            samples_teacher = diffusion.ddim_sample_loop_progressive_intermediate(\n",
    "                                                    model.forward_with_cfg, z.shape, noise=samples_student, clip_denoised=False, \n",
    "                                                    model_kwargs=model_kwargs, progress=False, device=device, step=steps*2+1)   \n",
    "                                        \n",
    "                                            samples_teacher_x0, _ = samples_teacher.chunk(2, dim=0)\n",
    "                                            samples_teacher.detach()\n",
    "                                            \n",
    "                                        with torch.enable_grad():    \n",
    "\n",
    "                                            # # AUTOCAST:\n",
    "                                            # signal = at\n",
    "                                            # noise = 1 - at\n",
    "                                            # log_snr = torch.log(signal / noise)\n",
    "                                            # weight = max(log_snr, 1)\n",
    "                                            # loss = weight * criterion(pred_x0_student, pred_x0_teacher.detach())\n",
    "                                            loss = criterion(samples_student_x0, samples_teacher_x0.detach())\n",
    "                                            # loss = criterion(samples_student[0], samples_teacher[0].detach())\n",
    "                                            scaler.scale(loss).backward()\n",
    "                                            scaler.step(optimizer)\n",
    "                                            scaler.update()\n",
    "                                            # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                            \n",
    "                                            # scheduler.step()\n",
    "                                            losses.append(loss.item())\n",
    "\n",
    "                                            torch.cuda.empty_cache()\n",
    "                                            # # NO AUTOCAST:\n",
    "                                            # # signal = at\n",
    "                                            # # noise = 1 - at\n",
    "                                            # # log_snr = torch.log(signal / noise)\n",
    "                                            # # weight = max(log_snr, 1)\n",
    "                                            # # loss = criterion(samples_student, samples_teacher.detach())\n",
    "                                            # loss = criterion(samples_student_x0, samples_teacher_x0.detach())\n",
    "                                            # # loss = criterion(pred_x0_student, pred_x0_teacher.detach())\n",
    "                                            # loss.backward()\n",
    "                                            # optimizer.step()\n",
    "                                            # # scheduler.step()\n",
    "                                            # # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                            \n",
    "                                            # losses.append(loss.item())\n",
    "                                            \n",
    "                                            \n",
    "                                        # if session != None and generation % 200 == 0 and generation > 0:\n",
    "                                                \n",
    "                                        #     x_T_teacher_decode = sampler_student.model.decode_first_stage(pred_x0_teacher)\n",
    "                                        #     teacher_target = torch.clamp((x_T_teacher_decode+1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        #     x_T_student_decode = sampler_student.model.decode_first_stage(pred_x0_student.detach())\n",
    "                                        #     student_target  = torch.clamp((x_T_student_decode +1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        #     predictions_temp.append(teacher_target)\n",
    "                                        #     predictions_temp.append(student_target)\n",
    "                                            \n",
    "                                        \n",
    "                                    \n",
    "\n",
    "                                        # if session != None and instance % 10000 == 0 and generation > 0:\n",
    "                                        #     fids = util.get_fid(student, sampler_student, num_imgs=100, name=run_name, instance = instance+1, steps=[64, 32, 16, 8, 4, 2, 1])\n",
    "                                        #     session.log({\"fid_64\":fids[0]})\n",
    "                                        #     session.log({\"fid_32\":fids[1]})\n",
    "                                        #     session.log({\"fid_16\":fids[2]})\n",
    "                                        #     session.log({\"fid_8\":fids[3]})\n",
    "                                        #     session.log({\"fid_4\":fids[4]})\n",
    "                                        #     session.log({\"fid_2\":fids[5]})\n",
    "                                        #     session.log({\"fid_1\":fids[6]})\n",
    "                                        \n",
    "                #                         if session != None and instance % 2000 == 0:\n",
    "                                            \n",
    "                #                             with torch.no_grad():\n",
    "                #                                 images, _ = util.compare_teacher_student(original, sampler_original, student, sampler_student, steps=[64, 32, 16, 8,  4, 2, 1], prompt=992)\n",
    "                #                                 images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                #                                 wandb.log({\"pred_x0\": images})\n",
    "                #                                 # images, _ = util.compare_teacher_student_with_schedule(original, sampler_original, student, sampler_student, steps=[64, 32, 16, 8,  4, 2, 1], prompt=992)\n",
    "                #                                 # images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                #                                 # wandb.log({\"schedule\": images})\n",
    "                #                                 sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "                #                                 sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "                #             if generation > 0 and generation % 20 == 0 and ddim_steps_student != 1 and step_scheduler==\"FID\":\n",
    "                #                 fid = util.get_fid(student, sampler_student, num_imgs=100, name=run_name, \n",
    "                #                             instance = instance, steps=[ddim_steps_student])\n",
    "                #                 if fid[0] <= current_fid[0] * 0.9 and decrease_steps==True:\n",
    "                #                     print(fid[0], current_fid[0])\n",
    "                #                     if ddim_steps_student in [16, 8, 4, 2, 1]:\n",
    "                #                         name = \"intermediate\"\n",
    "                #                         saving_loading.save_model(sampler_student, optimizer, scheduler, name, steps * 2, run_name)\n",
    "                #                     if ddim_steps_student != 2:\n",
    "                #                         ddim_steps_student -= 2\n",
    "                #                         updates -= 1\n",
    "                #                     else:\n",
    "                #                         ddim_steps_student = 1\n",
    "                #                         updates = 1    \n",
    "                #                     current_fid = fid\n",
    "                #                     print(\"steps decreased:\", ddim_steps_student)    \n",
    "\n",
    "                #             if session != None:\n",
    "                #                 with torch.no_grad():\n",
    "                #                     if session != None and generation % 200 == 0 and generation > 0:\n",
    "                #                         img, grid = util.compare_latents(predictions_temp)\n",
    "                #                         images = wandb.Image(grid, caption=\"left: Teacher, right: Student\")\n",
    "                #                         wandb.log({\"Inter_Comp\": images})\n",
    "                #                         del img, grid, predictions_temp, x_T_student_decode, x_T_teacher_decode, student_target, teacher_target\n",
    "                #                         torch.cuda.empty_cache()\n",
    "                            \n",
    "                            all_losses.extend(losses)\n",
    "                            averaged_losses.append(sum(losses) / len(losses))\n",
    "                            print(averaged_losses[-1])\n",
    "                            tepoch.set_postfix(epoch_loss=averaged_losses[-1])\n",
    "\n",
    "                # if step_scheduler == \"naive\" or \"gradual\" in step_scheduler:\n",
    "                #     util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "\n",
    "num_sampling_steps = 4\n",
    "lr = 0.001\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)#, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "# optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "# self_distillation_dit(model, original, optimizer, step_scheduler=\"naive\")\n",
    "self_distillation_dit(model, optimizer, step_scheduler=\"naive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "# import torch.nn as nn\n",
    "# scaler = GradScaler()\n",
    "\n",
    "\n",
    "# def self_distillation_dit(model, original, optimizer,\n",
    "#             session=None, steps=20, generations=200, early_stop=True, run_name=\"test\", decrease_steps=False,\n",
    "#             step_scheduler=\"deterministic\", type=\"snellius\"):\n",
    "#     \"\"\"\n",
    "#     Distill a model into itself. This is done by having a (teacher) model distill knowledge into itself. Copies of the original model and sampler \n",
    "#     are passed in to compare the original untrained version with the distilled model at scheduled intervals.\n",
    "#     \"\"\"\n",
    "#     NUM_CLASSES = 1000\n",
    "#     gradient_updates = generations\n",
    "#     ddim_steps_student = steps\n",
    "#     TEACHER_STEPS = 2\n",
    "#     ddim_eta = 0.0\n",
    "#     scale = 3.0\n",
    "#     optimizer=optimizer\n",
    "#     averaged_losses = []\n",
    "#     criterion = nn.MSELoss()\n",
    "#     instance = 0\n",
    "#     generation = 0\n",
    "#     all_losses = []\n",
    "#     num_sampling_steps = 64\n",
    "\n",
    "#     if step_scheduler == \"iterative\":\n",
    "#         halvings = math.floor(math.log(64)/math.log(2))\n",
    "#         updates_per_halving = int(gradient_updates / halvings)\n",
    "#         step_sizes = []\n",
    "#         for i in range(halvings):\n",
    "#             step_sizes.append(int((steps) / (2**i)))\n",
    "#         update_list = []\n",
    "#         for i in step_sizes:\n",
    "#             update_list.append(int(updates_per_halving / int(i/ 2)))\n",
    "#     elif step_scheduler == \"naive\":\n",
    "#         step_sizes=[ddim_steps_student]\n",
    "#         update_list=[gradient_updates // int(ddim_steps_student / 2)]\n",
    "#     elif step_scheduler == \"gradual_linear\":\n",
    "#         step_sizes = np.arange(steps, 0, -2)\n",
    "#         update_list = (1/len(np.append(step_sizes[1:], 1)) * gradient_updates / np.append(step_sizes[1:], 1)).astype(int)\n",
    "#     elif step_scheduler == \"gradual_exp\":\n",
    "#         step_sizes = np.arange(64, 0, -2)\n",
    "#         update_list = np.exp(1 / np.append(step_sizes[1:],1)) / np.sum(np.exp(1 / np.append(step_sizes[1:],1)))\n",
    "#         update_list = (update_list * gradient_updates /  np.append(step_sizes[1:],1)).astype(int)\n",
    "\n",
    "\n",
    "#     diffusion = create_diffusion(str(num_sampling_steps))\n",
    "#     diffusion_original = create_diffusion(str(num_sampling_steps)) \n",
    "#     n = 1\n",
    "#     num_sampling_steps =4 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "#     cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "#     samples_per_row = 1 #@param {type:\"number\"}\n",
    "#     indices = list(range(diffusion.num_timesteps))[::-1]\n",
    "#     with torch.no_grad():\n",
    "#         # with student.ema_scope():              \n",
    "              \n",
    "\n",
    "#                 for i, step in enumerate(step_sizes):\n",
    "#                     # if instance != 0 and \"gradual\" not in step_scheduler:\n",
    "#                     #     util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "#                     updates = int(step / 2)\n",
    "#                     generations = update_list[i]\n",
    "#                     print(\"Distilling to:\", updates)\n",
    "                         \n",
    "                    \n",
    "#                     # sc = student.get_learned_conditioning({student.cond_stage_key: torch.tensor(1*[1000]).to(student.device)})\n",
    "                    \n",
    "                    \n",
    "#                     with tqdm.tqdm(torch.randint(0, NUM_CLASSES, (generations,))) as tepoch:\n",
    "\n",
    "#                         for i, class_prompt in enumerate(tepoch):\n",
    "#                             generation += 1\n",
    "#                             losses = []        \n",
    "#                             class_labels = torch.tensor([class_prompt])\n",
    "#                             z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "#                             y = torch.tensor(class_labels, device=device)\n",
    "#                             z = torch.cat([z, z], 0)\n",
    "#                             y_null = torch.tensor([1000] * n, device=device)\n",
    "#                             y = torch.cat([y, y_null], 0)\n",
    "#                             model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "#                             # teacher_kwargs = model_kwargs.copy()\n",
    "#                             samples_ddim= None\n",
    "#                             predictions_temp = []\n",
    "                            \n",
    "#                             samples_teacher = torch.randn(z.shape, device=device)\n",
    "                                \n",
    "#                             for steps in range(updates):  \n",
    "#                                         optimizer.zero_grad()\n",
    "#                                     # with autocast():\n",
    "\n",
    "#                                         with torch.enable_grad():\n",
    "                                            \n",
    "#                                             instance += 1\n",
    "                                        \n",
    "                                        \n",
    "#                                             samples_student = diffusion.ddim_sample_loop_progressive_intermediate(\n",
    "#                                                     model.forward_with_cfg, z.shape, noise=samples_teacher, clip_denoised=False, \n",
    "#                                                     model_kwargs=model_kwargs, progress=False, device=device, step=steps*2, student=True)\n",
    "#                                             samples_student_x0, _ = samples_student.chunk(2, dim=0)\n",
    "#                                             # samples_student_x0, _ = samples.chunk(2, dim=0)\n",
    "\n",
    "#                                         with torch.no_grad():\n",
    "                                            \n",
    "#                                             samples_teacher = diffusion_original.ddim_sample_loop_progressive_intermediate(\n",
    "#                                                     original.forward_with_cfg, z.shape, noise=samples_student, clip_denoised=False, \n",
    "#                                                     model_kwargs=model_kwargs, progress=False, device=device, step=steps*2+1)   \n",
    "                                        \n",
    "#                                             samples_teacher_x0, _ = samples_teacher.chunk(2, dim=0)\n",
    "                                    \n",
    "#                                         with torch.enable_grad():    \n",
    "#                                             print(samples_student.requires_grad)\n",
    "#                                             # # AUTOCAST:\n",
    "#                                             # signal = at\n",
    "#                                             # noise = 1 - at\n",
    "#                                             # log_snr = torch.log(signal / noise)\n",
    "#                                             # weight = max(log_snr, 1)\n",
    "#                                             # loss = weight * criterion(pred_x0_student, pred_x0_teacher.detach())\n",
    "#                                             # scaler.scale(loss).backward()\n",
    "#                                             # scaler.step(optimizer)\n",
    "#                                             # scaler.update()\n",
    "#                                             # # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                            \n",
    "#                                             # scheduler.step()\n",
    "#                                             # losses.append(loss.item())\n",
    "\n",
    "                                            \n",
    "#                                             # NO AUTOCAST:\n",
    "#                                             # signal = at\n",
    "#                                             # noise = 1 - at\n",
    "#                                             # log_snr = torch.log(signal / noise)\n",
    "#                                             # weight = max(log_snr, 1)\n",
    "#                                             # loss = criterion(samples_student, samples_teacher.detach())\n",
    "#                                             loss = criterion(samples_student_x0, samples_teacher_x0.detach())\n",
    "#                                             # loss = criterion(pred_x0_student, pred_x0_teacher.detach())\n",
    "#                                             loss.backward()\n",
    "#                                             optimizer.step()\n",
    "#                                             # scheduler.step()\n",
    "#                                             # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                            \n",
    "#                                             losses.append(loss.item())\n",
    "                                            \n",
    "#                                         # if session != None and generation % 200 == 0 and generation > 0:\n",
    "                                                \n",
    "#                                         #     x_T_teacher_decode = sampler_student.model.decode_first_stage(pred_x0_teacher)\n",
    "#                                         #     teacher_target = torch.clamp((x_T_teacher_decode+1.0)/2.0, min=0.0, max=1.0)\n",
    "#                                         #     x_T_student_decode = sampler_student.model.decode_first_stage(pred_x0_student.detach())\n",
    "#                                         #     student_target  = torch.clamp((x_T_student_decode +1.0)/2.0, min=0.0, max=1.0)\n",
    "#                                         #     predictions_temp.append(teacher_target)\n",
    "#                                         #     predictions_temp.append(student_target)\n",
    "                                            \n",
    "                                        \n",
    "                                    \n",
    "\n",
    "#                                         # if session != None and instance % 10000 == 0 and generation > 0:\n",
    "#                                         #     fids = util.get_fid(student, sampler_student, num_imgs=100, name=run_name, instance = instance+1, steps=[64, 32, 16, 8, 4, 2, 1])\n",
    "#                                         #     session.log({\"fid_64\":fids[0]})\n",
    "#                                         #     session.log({\"fid_32\":fids[1]})\n",
    "#                                         #     session.log({\"fid_16\":fids[2]})\n",
    "#                                         #     session.log({\"fid_8\":fids[3]})\n",
    "#                                         #     session.log({\"fid_4\":fids[4]})\n",
    "#                                         #     session.log({\"fid_2\":fids[5]})\n",
    "#                                         #     session.log({\"fid_1\":fids[6]})\n",
    "                                        \n",
    "#                 #                         if session != None and instance % 2000 == 0:\n",
    "                                            \n",
    "#                 #                             with torch.no_grad():\n",
    "#                 #                                 images, _ = util.compare_teacher_student(original, sampler_original, student, sampler_student, steps=[64, 32, 16, 8,  4, 2, 1], prompt=992)\n",
    "#                 #                                 images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "#                 #                                 wandb.log({\"pred_x0\": images})\n",
    "#                 #                                 # images, _ = util.compare_teacher_student_with_schedule(original, sampler_original, student, sampler_student, steps=[64, 32, 16, 8,  4, 2, 1], prompt=992)\n",
    "#                 #                                 # images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "#                 #                                 # wandb.log({\"schedule\": images})\n",
    "#                 #                                 sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "#                 #                                 sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "#                 #             if generation > 0 and generation % 20 == 0 and ddim_steps_student != 1 and step_scheduler==\"FID\":\n",
    "#                 #                 fid = util.get_fid(student, sampler_student, num_imgs=100, name=run_name, \n",
    "#                 #                             instance = instance, steps=[ddim_steps_student])\n",
    "#                 #                 if fid[0] <= current_fid[0] * 0.9 and decrease_steps==True:\n",
    "#                 #                     print(fid[0], current_fid[0])\n",
    "#                 #                     if ddim_steps_student in [16, 8, 4, 2, 1]:\n",
    "#                 #                         name = \"intermediate\"\n",
    "#                 #                         saving_loading.save_model(sampler_student, optimizer, scheduler, name, steps * 2, run_name)\n",
    "#                 #                     if ddim_steps_student != 2:\n",
    "#                 #                         ddim_steps_student -= 2\n",
    "#                 #                         updates -= 1\n",
    "#                 #                     else:\n",
    "#                 #                         ddim_steps_student = 1\n",
    "#                 #                         updates = 1    \n",
    "#                 #                     current_fid = fid\n",
    "#                 #                     print(\"steps decreased:\", ddim_steps_student)    \n",
    "\n",
    "#                 #             if session != None:\n",
    "#                 #                 with torch.no_grad():\n",
    "#                 #                     if session != None and generation % 200 == 0 and generation > 0:\n",
    "#                 #                         img, grid = util.compare_latents(predictions_temp)\n",
    "#                 #                         images = wandb.Image(grid, caption=\"left: Teacher, right: Student\")\n",
    "#                 #                         wandb.log({\"Inter_Comp\": images})\n",
    "#                 #                         del img, grid, predictions_temp, x_T_student_decode, x_T_teacher_decode, student_target, teacher_target\n",
    "#                 #                         torch.cuda.empty_cache()\n",
    "                            \n",
    "#                             all_losses.extend(losses)\n",
    "#                             averaged_losses.append(sum(losses) / len(losses))\n",
    "#                             print(averaged_losses[-1])\n",
    "#                             tepoch.set_postfix(epoch_loss=averaged_losses[-1])\n",
    "\n",
    "#                 # if step_scheduler == \"naive\" or \"gradual\" in step_scheduler:\n",
    "#                 #     util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "\n",
    "# num_sampling_steps = 4\n",
    "# lr = 0.001\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "# # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "# self_distillation_dit(model, original, optimizer, step_scheduler=\"naive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1\n",
    "# num_sampling_steps =4 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "# cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "# samples_per_row = 1 #@param {type:\"number\"}\n",
    "# indices = list(range(diffusion.num_timesteps))[::-1]\n",
    "# class_labels = torch.tensor([992])\n",
    "# z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "# y = torch.tensor(class_labels, device=device)\n",
    "# z = torch.cat([z, z], 0)\n",
    "# y_null = torch.tensor([1000] * n, device=device)\n",
    "# y = torch.cat([y, y_null], 0)\n",
    "# model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "# # Sample images:\n",
    "# samples = diffusion.ddim_sample_loop_progressive(\n",
    "#     model.forward_with_cfg, z.shape, z, clip_denoised=False, \n",
    "#     model_kwargs=model_kwargs, progress=True, device=device\n",
    "# )\n",
    "# samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "# samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# # Save and display images:\n",
    "# save_image(samples, \"sample.png\", nrow=int(samples_per_row), \n",
    "#            normalize=True, value_range=(-1, 1))\n",
    "# samples = Image.open(\"sample.png\")\n",
    "# display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample images:\n",
    "# samples = diffusion_original.ddim_sample_loop_progressive(\n",
    "#     original.forward_with_cfg, z.shape, z, clip_denoised=False, \n",
    "#     model_kwargs=model_kwargs, progress=True, device=device\n",
    "# )\n",
    "# samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "# samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# # Save and display images:\n",
    "# save_image(samples, \"sample.png\", nrow=int(samples_per_row), \n",
    "#            normalize=True, value_range=(-1, 1))\n",
    "# samples_orig = Image.open(\"sample.png\")\n",
    "# display(samples_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples == samples_orig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
