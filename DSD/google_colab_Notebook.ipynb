{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")\n",
    "%cd /content/gdrive/MyDrive/Thesis/Diffusion_Thesis/cin_256\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning\n",
    "!pip install transformers==4.19.2 scann kornia==0.6.4 torchmetrics==0.7.0\n",
    "!pip install git+https://github.com/arogozhnikov/einops.git\n",
    "!pip install wandb\n",
    "!pip install omegaconf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/Thesis/Diffusion_Thesis/cin_256\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/gdrive/MyDrive/Thesis/Diffusion_Thesis/cin_256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Administrator/.netrc\n"
     ]
    }
   ],
   "source": [
    "from util_celeb import *\n",
    "import wandb\n",
    "\n",
    "cwd = os.getcwd()\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "config_path=f\"{cwd}/models/configs/celebahq-ldm-vq-4.yaml\"\n",
    "model_path=f\"{cwd}/models/CelebA.ckpt\"\n",
    "\n",
    "!wandb login 4baa24c4fc6c8eed782cacb721d34977149d4fcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teacher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mdel\u001b[39;00m teacher, sampler_teacher, student, sampler_student, optimizer, scheduler\n\u001b[0;32m      2\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'teacher' is not defined"
     ]
    }
   ],
   "source": [
    "del teacher, sampler_teacher, student, sampler_student, optimizer, scheduler\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create teacher and student model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from d:\\OneDrive\\Uni\\Master_Project_AI\\Code\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "teacher, sampler_teacher, student, sampler_student = create_models(config_path, model_path, student=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the student or teacher:\n",
    "\n",
    "(setting student=False will only return a single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from d:\\OneDrive\\Uni\\Master_Project_AI\\Code\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "# teacher, sampler_teacher = create_models(config_path, model_path, student=False)\n",
    "student, sampler_student = create_models(config_path, model_path, student=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading trained student as teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "model_path_test = f\"{cwd}/data/trained_models/10/student_intermediate_15.pt\"\n",
    "teacher, sampler_teacher, optimizer, scheduler = load_trained(model_path_test, config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WITHOUT intermediates saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, x_T_copy, class_prompt, intermediates  = generate(teacher, sampler_teacher, steps=128, scale=3, keep_intermediates=False)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating WITH intermediates saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediates, starting_noise, class_prompt = return_intermediates_for_student(teacher, sampler_teacher, steps=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_to_img(teacher, intermediates[125])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:26:03<00:00, 10.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# dataset_path = f\"{cwd}/data/pre_generated/teacher_128/\"\n",
    "# dataset_name = \"teacher_128.pt\"\n",
    "# make_dataset(teacher, sampler_teacher, 501, 128, dataset_path, dataset_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training student from teacher directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_session = wandb_log(name=\"test_loss\", lr=1e-8, model=student, tags=[\"test\"], notes=\"Just a test\")\n",
    "\n",
    "optimizer, scheduler = get_optimizer(sampler_student, iterations=20*20)\n",
    "teacher_train_student(teacher, sampler_teacher, student, sampler_student, optimizer, scheduler, steps=20, generations=20, early_stop=True, session=wandb_session)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra, quick comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = compare_teacher_student(teacher, sampler_teacher, student, sampler_student, steps=[1, 2, 4, 8, 16, 32, 64])\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_celeb import *\n",
    "import wandb\n",
    "\n",
    "cwd = os.getcwd()\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# config_path=f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "# model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "\n",
    "config_path=f\"{cwd}/models/configs/celebahq-ldm-vq-4.yaml\"\n",
    "model_path=f\"{cwd}/models/CelebA.ckpt\"\n",
    "\n",
    "\n",
    "teacher, sampler_teacher, student, sampler_student = create_models(config_path, model_path, student=True)\n",
    "\n",
    "optimizer, scheduler = get_optimizer(sampler_student, iterations=32*10)\n",
    "teacher_train_student(teacher, sampler_teacher, student, sampler_student, optimizer, scheduler, steps=32, generations=50, early_stop=False, session=None)\n",
    "\n",
    "images, _ = compare_teacher_student(teacher, sampler_teacher, student, sampler_student, steps=[1, 2, 4, 8, 16, 32])\n",
    "\n",
    "\n",
    "del teacher, sampler_teacher, student, sampler_student, optimizer, scheduler\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del teacher, sampler_teacher, student, sampler_student, optimizer, scheduler\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train student from pre-generated teacher dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"{cwd}/data/pre_generated/teacher_128/\"\n",
    "datasets = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = \"\"\"This is a serious attempt to distill the 128 step original teacher into a 64 step student, trained on 32000 instances\"\"\"\n",
    "wandb_session = wandb_log(name=\"Train_student_on_128_pretrained\", lr=0.00000001, model=student, tags=[\"distillation\"], notes=notes)\n",
    "\n",
    "optimizer, scheduler = get_optimizer(sampler_student, iterations=64*500)\n",
    "\n",
    "dataset = torch.load(data_dir + datasets[0])\n",
    "train_student_from_dataset(student, sampler_student, dataset, 64, optimizer, scheduler, early_stop=True, session=wandb_session)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "save_model(sampler_student, optimizer, scheduler, name=\"lr8_scheduled\", steps=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Loading trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(sampler_student, optimizer, scheduler, name=\"lr8_scheduled\", steps=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{cwd}/data/trained_models/64/student_lr8_scheduled.pt\"\n",
    "student, sampler_student, optimizer, scheduler = load_trained(path, config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = compare_teacher_student(teacher, sampler_teacher, student, sampler_student, steps=[1, 2, 4, 8, 16, 32, 64, 128])\n",
    "images\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Loop (allnighter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Administrator/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/CelebA.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 274.06 M params.\n",
      "Keeping EMAs of 370.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n",
      "Training LatentDiffusion as an unconditional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpmobiluss\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Diffusion_Thesis\\cin_256\\wandb\\run-20230312_155843-c68vr9s3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pmobiluss/diffusion-thesis/runs/c68vr9s3' target=\"_blank\">Train_student_on_128_pretrained</a></strong> to <a href='https://wandb.ai/pmobiluss/diffusion-thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pmobiluss/diffusion-thesis' target=\"_blank\">https://wandb.ai/pmobiluss/diffusion-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pmobiluss/diffusion-thesis/runs/c68vr9s3' target=\"_blank\">https://wandb.ai/pmobiluss/diffusion-thesis/runs/c68vr9s3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/78 [01:33<28:46, 23.33s/it, epoch_loss=0.00739, lr=[9.935251313189571e-07]]"
     ]
    }
   ],
   "source": [
    "from util_celeb import *\n",
    "import wandb\n",
    "\n",
    "cwd = os.getcwd()\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!wandb login 4baa24c4fc6c8eed782cacb721d34977149d4fcb\n",
    "\n",
    "ddim_steps = [16, 8, 4, 2]\n",
    "generations = 10000\n",
    "config=f\"{cwd}/models/configs/celebahq-ldm-vq-4.yaml\"\n",
    "original_model_path=f\"{cwd}/models/CelebA.ckpt\"\n",
    "\n",
    "run_name = \"Colab_2000_1e-6\"\n",
    "tags = [\"Colab\"]\n",
    "\n",
    "distill(ddim_steps, generations, run_name, config, original_model_path, lr=0.0001, tags=tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
