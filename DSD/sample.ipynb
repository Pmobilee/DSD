{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/damion/Code/DSD/DSD/models/lsun_bedrooms.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damion/miniconda3/envs/D-SD/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 274.06 M params.\n",
      "Keeping EMAs of 370.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 32, 32) = 3072 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n",
      "Training LatentDiffusion as an unconditional model.\n",
      "Loading model from /home/damion/Code/DSD/DSD/models/lsun_bedrooms.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 274.06 M params.\n",
      "Keeping EMAs of 370.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 32, 32) = 3072 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n",
      "Training LatentDiffusion as an unconditional model.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import self_distillation\n",
    "import distillation\n",
    "import saving_loading\n",
    "import generate\n",
    "import wandb\n",
    "import util\n",
    "import os\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from IPython.display import display, clear_output\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.util import *\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import wandb\n",
    "import math\n",
    "import traceback\n",
    "from pytorch_fid import fid_score\n",
    "import shutil\n",
    "import util\n",
    "import saving_loading\n",
    "import generate\n",
    "\n",
    "gradient_updates = 4000\n",
    "lr = 0.0001\n",
    "steps = 8\n",
    "step_scheduler = \"naive\"\n",
    "x0 = True\n",
    "warmup_epochs = 1000\n",
    "cwd = os.getcwd()\n",
    "\n",
    "model_path=f\"{cwd}/models/lsun_bedrooms.ckpt\"\n",
    "config_path = f\"{cwd}/models/configs/lsun_bedrooms-ldm-vq-4.yaml\"\n",
    "\n",
    "# model_path=f\"{cwd}/models/cin256_retrained.pt\"\n",
    "# config_path = f\"{cwd}/models/configs/cin256-v2-custom_x0.yaml\"\n",
    "\n",
    "student, sampler_student = util.create_models(config_path, model_path, student=False)\n",
    "original, sampler_original = util.create_models(config_path, model_path, student=False)\n",
    "optimizer, scheduler = util.get_optimizer(sampler_student, iterations=gradient_updates, warmup_epochs=warmup_epochs,lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling to: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4865]]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.76 GiB total capacity; 9.31 GiB already allocated; 109.44 MiB free; 9.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/damion/Code/DSD/DSD/sample.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W1sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m loss \u001b[39m=\u001b[39m weight \u001b[39m*\u001b[39m criterion(pred_x0_student, pred_x0_teacher\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W1sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W1sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m scaler\u001b[39m.\u001b[39;49mstep(optimizer)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W1sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m scaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W1sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(sampler_student\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/D-SD/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:374\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    372\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 374\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    376\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[1;32m    378\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/D-SD/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:290\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 290\u001b[0m     retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mstep(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/D-SD/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/D-SD/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/D-SD/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/D-SD/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/D-SD/lib/python3.8/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/D-SD/lib/python3.8/site-packages/torch/optim/adam.py:442\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    439\u001b[0m torch\u001b[39m.\u001b[39m_foreach_add_(device_state_steps, \u001b[39m1\u001b[39m)\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 442\u001b[0m     device_grads \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_add(device_grads, device_params, alpha\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[1;32m    444\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    445\u001b[0m torch\u001b[39m.\u001b[39m_foreach_mul_(device_exp_avgs, beta1)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.76 GiB total capacity; 9.31 GiB already allocated; 109.44 MiB free; 9.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "\n",
    "NUM_CLASSES = 1000\n",
    "ddim_steps_student = 64 # Setting the number of steps for the student model\n",
    "ddim_eta = 1.0 \n",
    "# For both the student and the original model, the number of steps is set to the same value. \n",
    "# Technically the original model does not need to be trained, but it is kept for comparison purposes.\n",
    "sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "# Setting the eta value to 0.0 means a deterministic output given the original noise, essential\n",
    "scale = 3.0 # This is $w$ in the paper, the CFG scale. Can be left static or varied as is done occasionally.\n",
    "criterion = nn.MSELoss() \n",
    "\n",
    "instance = 0 # Actual instance of student gradient updates\n",
    "generation = 0 # The amount of final-step images generated\n",
    "averaged_losses = []\n",
    "all_losses = []\n",
    "\n",
    "if step_scheduler == \"iterative\": # Halve the number of steps from start to 1 with even allocation of gradient updates\n",
    "    halvings = math.floor(math.log(ddim_steps_student)/math.log(2))\n",
    "    updates_per_halving = int(gradient_updates / halvings)\n",
    "    step_sizes = []\n",
    "    for i in range(halvings):\n",
    "        step_sizes.append(int((steps) / (2**i)))\n",
    "    update_list = []\n",
    "    for i in step_sizes:\n",
    "        update_list.append(int(updates_per_halving / int(i/ 2))) # /2 because of 2 steps per update\n",
    "elif step_scheduler == \"naive\": # Naive approach, evenly distribute gradient updates over all steps\n",
    "    step_sizes=[ddim_steps_student]\n",
    "    update_list=[gradient_updates // int(ddim_steps_student / 2)] # /2 because of 2 steps per update\n",
    "elif step_scheduler == \"gradual_linear\": # Gradually decrease the number of steps to 1, with even allocation of gradient updates\n",
    "    step_sizes = np.arange(steps, 0, -2)\n",
    "    update_list = ((1/len(np.append(step_sizes[1:], 1)) * gradient_updates / np.append(step_sizes[1:], 1))).astype(int) * 2 # *2 because of 2 steps per update\n",
    "elif step_scheduler == \"gradual_exp\": # TEMPORARY VERSION, to test if focus on higher steps is better, reverse of the one below\n",
    "    step_sizes = np.arange(64, 0, -2)\n",
    "    update_list = np.exp((1 / np.append(step_sizes[1:],1))[::-1]) / np.sum(np.exp((1 / np.append(step_sizes[1:],1))[::-1]))\n",
    "    update_list = (update_list * gradient_updates /  np.append(step_sizes[1:],1)).astype(int) * 2 # *2 because of 2 steps per update\n",
    "# elif step_scheduler == \"gradual_exp\": # Exponential decrease in number of gradient updates per step\n",
    "#     step_sizes = np.arange(64, 0, -2)\n",
    "#     update_list = np.exp(1 / np.append(step_sizes[1:],1)) / np.sum(np.exp(1 / np.append(step_sizes[1:],1)))\n",
    "#     update_list = ((update_list * 2) * gradient_updates /  np.append(step_sizes[1:],1)).astype(int)\n",
    "\n",
    "# Weird structure for gradient calculations in an  attempt to save memory, Celeb wants a lot for some reason\n",
    "with torch.no_grad():\n",
    "    with student.ema_scope():              \n",
    "    \n",
    "        for i, step in enumerate(step_sizes):\n",
    "            if instance != 0 and \"gradual\" not in step_scheduler:   # Save the model after every step size. Given the large model size, \n",
    "                                                                    # the gradual versions are not saved each time (steps * 2 * 4.7gb is a lot!)\n",
    "                util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "            updates = int(step / 2) # We take updates as half the step size, because we do 2 steps per update\n",
    "            generations = update_list[i] # The number of generations has been determined earlier\n",
    "            print(\"Distilling to:\", updates)\n",
    "\n",
    "            with tqdm.tqdm(range(generations)) as tepoch:\n",
    "                for j in tepoch:\n",
    "                    generation += 1\n",
    "                    losses = []        \n",
    "        \n",
    "                    samples_ddim = None # Setting to None will create a new noise vector for each generation\n",
    "                    predictions_temp = []\n",
    "                    for steps in range(updates):  \n",
    "                        with autocast and torch.enable_grad(): # For mixed precision training, should not be used for final results\n",
    "                            instance += 1\n",
    "                            optimizer.zero_grad()\n",
    "                        \n",
    "                            samples_ddim, pred_x0_student, _, at, _= sampler_student.sample_student(S=1,\n",
    "                                                                conditioning=None,\n",
    "                                                                batch_size=1,\n",
    "                                                                shape=[3, 64, 64],\n",
    "                                                                verbose=False,\n",
    "                                                                x_T=samples_ddim, # start noise or teacher output\n",
    "                                                                unconditional_guidance_scale=scale,\n",
    "                                                                unconditional_conditioning=None, \n",
    "                                                                eta=ddim_eta,\n",
    "                                                                keep_intermediates=False,\n",
    "                                                                intermediate_step = steps*2,\n",
    "                                                                steps_per_sampling = 1,\n",
    "                                                                total_steps = step)\n",
    "                        \n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            \n",
    "                            samples_ddim, _, _, pred_x0_teacher, _ , _= sampler_student.sample(S=1,\n",
    "                                                            conditioning=None,\n",
    "                                                            batch_size=1,\n",
    "                                                            shape=[3, 64, 64],\n",
    "                                                            verbose=False,\n",
    "                                                            x_T=samples_ddim, #output of student\n",
    "                                                            unconditional_guidance_scale=scale,\n",
    "                                                            unconditional_conditioning=None, \n",
    "                                                            eta=ddim_eta,\n",
    "                                                            keep_intermediates=False,\n",
    "                                                            intermediate_step = steps*2+1,\n",
    "                                                            steps_per_sampling = 1,\n",
    "                                                            total_steps = step)     \n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                        with autocast and torch.enable_grad():    \n",
    "                        \n",
    "                            # AUTOCAST:\n",
    "                            signal = at\n",
    "                            noise = 1 - at\n",
    "                            log_snr = torch.log(signal / noise)\n",
    "                            weight = max(log_snr, 1)\n",
    "                            loss = weight * criterion(pred_x0_student, pred_x0_teacher.detach())\n",
    "                            scaler.scale(loss).backward()\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                            losses.append(loss.item())\n",
    "\n",
    "                            # # No AutoCast:\n",
    "                            # signal = at\n",
    "                            # noise = 1 - at\n",
    "                            # log_snr = torch.log(signal / noise)\n",
    "                            # weight = max(log_snr, 1)\n",
    "                            # loss = weight * criterion(pred_x0_student, pred_x0_teacher.detach())\n",
    "                            # loss.backward()\n",
    "                            # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                            # optimizer.step()\n",
    "                            # scheduler.step()\n",
    "                            \n",
    "                            # losses.append(loss.item())\n",
    "\n",
    "                        # if session != None and instance % 10000 == 0 and generation > 0:\n",
    "                        #     fids = util.get_fid_celeb(student, sampler_student, num_imgs=100, name=run_name, instance = instance+1, steps=[64, 32, 16, 8, 4, 2, 1])\n",
    "                        #     session.log({\"fid_64\":fids[0]})\n",
    "                        #     session.log({\"fid_32\":fids[1]})\n",
    "                        #     session.log({\"fid_16\":fids[2]})\n",
    "                        #     session.log({\"fid_8\":fids[3]})\n",
    "                        #     session.log({\"fid_4\":fids[4]})\n",
    "                        #     session.log({\"fid_2\":fids[5]})\n",
    "                        #     session.log({\"fid_1\":fids[6]})\n",
    "\n",
    "                    # if generation > 0 and generation % 20 == 0 and ddim_steps_student != 1 and step_scheduler==\"FID\":\n",
    "                    #     fid = util.get_fid(student, sampler_student, num_imgs=100, name=run_name, \n",
    "                    #                 instance = instance, steps=[ddim_steps_student])\n",
    "                    #     if fid[0] <= current_fid[0] * 0.9 and decrease_steps==True:\n",
    "                    #         print(fid[0], current_fid[0])\n",
    "                    #         if ddim_steps_student in [16, 8, 4, 2, 1]:\n",
    "                    #             name = \"intermediate\"\n",
    "                    #             saving_loading.save_model(sampler_student, optimizer, scheduler, name, steps * 2, run_name)\n",
    "                    #         if ddim_steps_student != 2:\n",
    "                    #             ddim_steps_student -= 2\n",
    "                    #             updates -= 1\n",
    "                    #         else:\n",
    "                    #             ddim_steps_student = 1\n",
    "                    #             updates = 1    \n",
    "                    #         current_fid = fid\n",
    "                    #         print(\"steps decresed:\", ddim_steps_student)    \n",
    "\n",
    "                    with torch.no_grad():\n",
    "                            if instance % 10 == 0:\n",
    "                                images, _ = util.compare_teacher_student_celeb(original, sampler_original, student, sampler_student, steps=[64, 32, 16, 8,  4, 2, 1])\n",
    "                                clear_output(wait=True)\n",
    "                                display(images)\n",
    "                    \n",
    "                    all_losses.extend(losses)\n",
    "                    averaged_losses.append(sum(losses) / len(losses))\n",
    "                    \n",
    "                    tepoch.set_postfix(epoch_loss=averaged_losses[-1])\n",
    "\n",
    "            if step_scheduler == \"naive\" or \"gradual\" in step_scheduler: # Save the final model, since we skipped all the intermediate steps\n",
    "                util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling to: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/damion/Code/DSD/DSD/sample.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m scale \u001b[39m=\u001b[39m \u001b[39m3.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m#scale = np.random.uniform(1.0, 4.0) # Randomly sample a scale for each generation, optional\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m c_student \u001b[39m=\u001b[39m student\u001b[39m.\u001b[39;49mget_learned_conditioning({student\u001b[39m.\u001b[39;49mcond_stage_key: torch\u001b[39m.\u001b[39;49mtensor([class_prompt])\u001b[39m.\u001b[39;49mto(student\u001b[39m.\u001b[39;49mdevice)}) \u001b[39m# Set to 0 for unconditional, requires pretraining\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W2sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m samples_ddim\u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m# Setting to None will create a new noise vector for each generation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/damion/Code/DSD/DSD/sample.ipynb#W2sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m predictions_temp \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Code/DSD/DSD/ldm/models/diffusion/ddpm.py:561\u001b[0m, in \u001b[0;36mLatentDiffusion.get_learned_conditioning\u001b[0;34m(self, c)\u001b[0m\n\u001b[1;32m    559\u001b[0m             c \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39mmode()\n\u001b[1;32m    560\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m         c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcond_stage_model(c)\n\u001b[1;32m    562\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcond_stage_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcond_stage_forward)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 1000\n",
    "ddim_steps_student = steps # Setting the number of steps for the student model\n",
    "ddim_eta = 0.0 # Setting the eta value to 0.0 means a deterministic output given the original noise, essential\n",
    "# For both the student and the original model, the number of steps is set to the same value. \n",
    "# Technically the original model does not need to be trained, but it is kept for comparison purposes.\n",
    "sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "ddim_eta = 0.0 # Setting the eta value to 0.0 means a deterministic output given the original noise, essential\n",
    "scale = 3.0 # This is $w$ in the paper, the CFG scale. Can be left static or varied as is done occasionally.\n",
    "criterion = nn.MSELoss() \n",
    "\n",
    "instance = 0 # Actual instance of student gradient updates\n",
    "generation = 0 # The amount of final-step images generated\n",
    "averaged_losses = []\n",
    "all_losses = []\n",
    "\n",
    "if step_scheduler == \"iterative\": # Halve the number of steps from start to 1 with even allocation of gradient updates\n",
    "    halvings = math.floor(math.log(ddim_steps_student)/math.log(2))\n",
    "    updates_per_halving = int(gradient_updates / halvings)\n",
    "    step_sizes = []\n",
    "    for i in range(halvings):\n",
    "        step_sizes.append(int((steps) / (2**i)))\n",
    "    update_list = []\n",
    "    for i in step_sizes:\n",
    "        update_list.append(int(updates_per_halving / int(i/ 2))) # /2 because of 2 steps per update\n",
    "elif step_scheduler == \"naive\": # Naive approach, evenly distribute gradient updates over all steps\n",
    "    step_sizes=[ddim_steps_student]\n",
    "    update_list=[gradient_updates // int(ddim_steps_student / 2)] # /2 because of 2 steps per update\n",
    "elif step_scheduler == \"gradual_linear\": # Gradually decrease the number of steps to 1, with even allocation of gradient updates\n",
    "    step_sizes = np.arange(steps, 0, -2)\n",
    "    update_list = ((1/len(np.append(step_sizes[1:], 1)) * gradient_updates / np.append(step_sizes[1:], 1))).astype(int) * 2 # *2 because of 2 steps per update\n",
    "elif step_scheduler == \"gradual_exp\": # TEMPORARY VERSION, to test if focus on higher steps is better, reverse of the one below\n",
    "    step_sizes = np.arange(64, 0, -2)\n",
    "    update_list = np.exp((1 / np.append(step_sizes[1:],1))[::-1]) / np.sum(np.exp((1 / np.append(step_sizes[1:],1))[::-1]))\n",
    "    update_list = (update_list * gradient_updates /  np.append(step_sizes[1:],1)).astype(int) * 2 # *2 because of 2 steps per update\n",
    "# elif step_scheduler == \"gradual_exp\": # Exponential decrease in number of gradient updates per step\n",
    "#     step_sizes = np.arange(64, 0, -2)\n",
    "#     update_list = np.exp(1 / np.append(step_sizes[1:],1)) / np.sum(np.exp(1 / np.append(step_sizes[1:],1)))\n",
    "#     update_list = ((update_list * 2) * gradient_updates /  np.append(step_sizes[1:],1)).astype(int)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # student.use_ema = True\n",
    "    # with student.ema_scope(): \n",
    "            if x0:\n",
    "                sc=None\n",
    "            else:\n",
    "                sc = student.get_learned_conditioning({student.cond_stage_key: torch.tensor(1*[1000]).to(student.device)}) # Get the learned conditioning\n",
    "            for i, step in enumerate(step_sizes): # For each step size\n",
    "                # if instance != 0 and \"gradual\" not in step_scheduler:   # Save the model after every step size. Given the large model size, \n",
    "                #                                                         # the gradual versions are not saved each time (steps * 2 * 4.7gb is a lot!)\n",
    "                #     util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "                updates = int(step / 2) # We take updates as half the step size, because we do 2 steps per update\n",
    "                generations = update_list[i] # The number of generations has been determined earlier\n",
    "                print(\"Distilling to:\", step)\n",
    "                \n",
    "                with tqdm.tqdm(torch.randint(0, NUM_CLASSES, (generations,))) as tepoch: # Take a random class for each generation\n",
    "\n",
    "                    for i, class_prompt in enumerate(tepoch):\n",
    "                        generation += 1\n",
    "                        losses = []       \n",
    "                        scale = 3.0\n",
    "                        #scale = np.random.uniform(1.0, 4.0) # Randomly sample a scale for each generation, optional\n",
    "                        c_student = student.get_learned_conditioning({student.cond_stage_key: torch.tensor([class_prompt]).to(student.device)}) # Set to 0 for unconditional, requires pretraining\n",
    "                        \n",
    "                        samples_ddim= None # Setting to None will create a new noise vector for each generation\n",
    "                        predictions_temp = []\n",
    "                        \n",
    "                        for steps in range(updates):\n",
    "                            # with autocast() and torch.enable_grad(): # For mixed precision training, should not be used for final results\n",
    "                                with torch.enable_grad():\n",
    "                                        instance += 1\n",
    "                                        \n",
    "                                        optimizer.zero_grad()\n",
    "                                        samples_ddim, pred_x0_student, _, at= sampler_student.sample_student(S=1,\n",
    "                                                                            conditioning=c_student,\n",
    "                                                                            batch_size=1,\n",
    "                                                                            shape=[3, 64, 64],\n",
    "                                                                            verbose=False,\n",
    "                                                                            x_T=samples_ddim, # start noise or teacher output\n",
    "                                                                            unconditional_guidance_scale=scale,\n",
    "                                                                            unconditional_conditioning=sc, \n",
    "                                                                            eta=ddim_eta,\n",
    "                                                                            keep_intermediates=False,\n",
    "                                                                            intermediate_step = steps*2,\n",
    "                                                                            steps_per_sampling = 1,\n",
    "                                                                            total_steps = ddim_steps_student)\n",
    "                                        \n",
    "                                        # Code below first decodes the latent image and then reconstructs it. This is not necessary, but can be used to check if the latent image is correct\n",
    "                                        # decode_student = student.differentiable_decode_first_stage(pred_x0_student)\n",
    "                                        # reconstruct_student = torch.clamp((decode_student+1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        \n",
    "                                        \n",
    "\n",
    "                                        with torch.no_grad():\n",
    "                                            samples_ddim.detach()\n",
    "                                            samples_ddim, _, _, pred_x0_teacher, _ = sampler_student.sample(S=1,\n",
    "                                                                        conditioning=c_student,\n",
    "                                                                        batch_size=1,\n",
    "                                                                        shape=[3, 64, 64],\n",
    "                                                                        verbose=False,\n",
    "                                                                        x_T=samples_ddim, # output of student\n",
    "                                                                        unconditional_guidance_scale=scale,\n",
    "                                                                        unconditional_conditioning=sc, \n",
    "                                                                        eta=ddim_eta,\n",
    "                                                                        keep_intermediates=False,\n",
    "                                                                        intermediate_step = steps*2+1,\n",
    "                                                                        steps_per_sampling = 1,\n",
    "                                                                        total_steps = ddim_steps_student)     \n",
    "\n",
    "                                            # decode_teacher = student.decode_first_stage(pred_x0_teacher)\n",
    "                                            # reconstruct_teacher = torch.clamp((decode_teacher+1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        signal = at\n",
    "                                        noise = 1 - at\n",
    "                                        log_snr = torch.log(signal / noise)\n",
    "                                        weight = max(log_snr, 1)\n",
    "                                        loss = weight *  criterion(pred_x0_student, pred_x0_teacher.detach())     \n",
    "                                        # loss = weight * criterion(reconstruct_student, reconstruct_teacher.detach())                    \n",
    "                                        loss.backward()\n",
    "                                        optimizer.step()\n",
    "                                        scheduler.step()\n",
    "                                        # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                        losses.append(loss.item())\n",
    "\n",
    "                                        \n",
    "                        if  generation > 0 and generation % 2 == 0: # or instance==1:\n",
    "\n",
    "                                with torch.no_grad():\n",
    "                                    # the x0 version keeps max denoising steps to 64\n",
    "                                    images, _ = util.compare_teacher_student_x0(original, sampler_original, student, sampler_student, steps=[8], prompt=992, x0=x0)\n",
    "                                    # images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                                    # wandb.log({\"pred_x0\": images})\n",
    "                                    clear_output(wait=True)\n",
    "                                    display(images)\n",
    "                                    # # Optional; compare the images but also change the denoising schedule\n",
    "                                    # images, _ = util.compare_teacher_student(original, sampler_original, student, sampler_student, steps=[64, 16, 8,  4, 2, 1], prompt=992,x0=x0)\n",
    "                                    # # images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                                    # # wandb.log({\"with_sched\": images})\n",
    "\n",
    "                                    # Important: Reset the schedule, as compare_teacher_student changes max steps. \n",
    "                                    sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "                                    sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D-SD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
