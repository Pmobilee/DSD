{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "import self_distillation\n",
    "import saving_loading\n",
    "import generate\n",
    "import wandb\n",
    "import util\n",
    "import os\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "cwd = os.getcwd()\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!wandb login 4baa24c4fc6c8eed782cacb721d34977149d4fcb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"Cin_256_custom.ipynb\"\n",
    "\n",
    "config_path=f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "\n",
    "# config_path=f\"{cwd}/models/configs/celebahq-ldm-vq-4.yaml\"\n",
    "# model_path=f\"{cwd}/models/CelebA.ckpt\"\n",
    "\n",
    "teacher, sampler_teacher = util.create_models(config_path, model_path, student=False)\n",
    "original, sampler_original = util.create_models(config_path, model_path, student=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'linear' in name:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': params_to_update, 'lr': 0.001},  # Parameters to update\n",
    "   # Other parameters (not updated)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, _ = util.compare_teacher_student_x0(teacher, sampler_teacher, original, sampler_original, steps=[128], prompt=[992])\n",
    "images, _ = util.compare_teacher_student(teacher, sampler_teacher, original, sampler_original, steps=[32], prompt=[992])\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "gradient_updates = 100000\n",
    "ddim_steps_student = 64\n",
    "steps = 64\n",
    "\n",
    "halvings = math.floor(math.log(64)/math.log(2))\n",
    "updates_per_halving = int(gradient_updates / halvings)\n",
    "step_sizes = []\n",
    "for i in range(halvings):\n",
    "    step_sizes.append(int((steps) / (2**i)))\n",
    "iterative = []\n",
    "for i in step_sizes:\n",
    "    iterative.append(int(updates_per_halving / int(i/ 2)))\n",
    "\n",
    "step_sizes=[ddim_steps_student]\n",
    "naive=[gradient_updates // int(ddim_steps_student / 2)] * int(ddim_steps_student / 2)\n",
    "\n",
    "\n",
    "step_sizes = np.arange(steps, 0, -2)\n",
    "gradual_linear = (1/len(np.append(step_sizes[1:], 1)) * gradient_updates / np.append(step_sizes[1:], 1)).astype(int)\n",
    "\n",
    "\n",
    "step_sizes = np.arange(64, 0, -2)\n",
    "update_list = np.exp(1 / np.append(step_sizes[1:],1)) / np.sum(np.exp(1 / np.append(step_sizes[1:],1)))\n",
    "gradual_exp = (update_list * gradient_updates /  np.append(step_sizes[1:],1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradual_exp = gradual_exp / np.sum(gradual_exp)\n",
    "gradual_linear = gradual_linear / np.sum(gradual_linear)\n",
    "naive = naive / np.sum(naive)\n",
    "iterative = iterative / np.sum(iterative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01584931, 0.0317291 , 0.06348868, 0.12697735, 0.25398519,\n",
       "       0.50797037])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125,\n",
       "       0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125,\n",
       "       0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125,\n",
       "       0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125,\n",
       "       0.03125, 0.03125, 0.03125, 0.03125])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (32,) and (6,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39mplot(step_sizes, gradual_linear, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgradual_linear\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(step_sizes, naive, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnaive\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m plt\u001b[39m.\u001b[39;49mplot(step_sizes, iterative, label\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39miterative\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[0;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[0;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (32,) and (6,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(step_sizes, gradual_exp, label=\"gradual_exp\")\n",
    "plt.plot(step_sizes, gradual_linear, label=\"gradual_linear\")\n",
    "plt.plot(step_sizes, naive, label=\"naive\")\n",
    "plt.plot(step_sizes, iterative, label=\"iterative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image, noise_list, prompt_list = generate.ablation(sampler_teacher, steps=16, shape=(5, 4))\n",
    "new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_image, noise_list, prompt_list \u001b[39m=\u001b[39m generate\u001b[39m.\u001b[39mablation(sampler_teacher, steps\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shape\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m4\u001b[39m), prompt_list\u001b[39m=\u001b[39mprompt_list, noise_list\u001b[39m=\u001b[39mnoise_list)\n\u001b[0;32m      2\u001b[0m new_image\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "new_image, noise_list, prompt_list = generate.ablation(sampler_teacher, steps=4, shape=(5, 4), prompt_list=prompt_list, noise_list=noise_list)\n",
    "new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.all(sampler_teacher.model.state_dict()[\"model.diffusion_model.output_blocks.2.0.in_layers.2.weight\"] == sampler_teacher.model.state_dict()[\"model.diffusion_model.output_blocks.2.0.in_layers.2.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [01:23<00:00,  1.41it/s]\n",
      "100%|██████████| 118/118 [01:36<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "227.723634554448"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_fid import fid_score\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "util.get_fid(teacher, sampler_teacher, num_imgs=500, name=\"tsd_best\", instance=4, steps=[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [01:47<00:00,  1.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81.13977821427395"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([f\"{cwd}\\celeb_64.npz\", \n",
    "                         f\"{cwd}\\saved_images\\celeb\\DSDN\\\\8\"], 256, dims=2048, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [01:46<00:00,  1.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81.33419639250224"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([f\"{cwd}\\celeb_64.npz\", \n",
    "                         f\"{cwd}\\saved_images\\celeb\\DSDI\\\\8\"], 256, dims=2048, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12/792 [00:12<13:58,  1.07s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fid_score\u001b[39m.\u001b[39;49mcalculate_fid_given_paths([\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mcwd\u001b[39m}\u001b[39;49;00m\u001b[39m\\\u001b[39;49m\u001b[39mimg_align_celeba\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m      2\u001b[0m                          \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mcwd\u001b[39m}\u001b[39;49;00m\u001b[39m\\\u001b[39;49m\u001b[39msaved_images\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mceleb\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mTSD\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m8\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m256\u001b[39;49m, dims\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\pytorch_fid\\fid_score.py:259\u001b[0m, in \u001b[0;36mcalculate_fid_given_paths\u001b[1;34m(paths, batch_size, device, dims, num_workers)\u001b[0m\n\u001b[0;32m    255\u001b[0m block_idx \u001b[39m=\u001b[39m InceptionV3\u001b[39m.\u001b[39mBLOCK_INDEX_BY_DIM[dims]\n\u001b[0;32m    257\u001b[0m model \u001b[39m=\u001b[39m InceptionV3([block_idx])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> 259\u001b[0m m1, s1 \u001b[39m=\u001b[39m compute_statistics_of_path(paths[\u001b[39m0\u001b[39;49m], model, batch_size,\n\u001b[0;32m    260\u001b[0m                                     dims, device, num_workers)\n\u001b[0;32m    261\u001b[0m m2, s2 \u001b[39m=\u001b[39m compute_statistics_of_path(paths[\u001b[39m1\u001b[39m], model, batch_size,\n\u001b[0;32m    262\u001b[0m                                     dims, device, num_workers)\n\u001b[0;32m    263\u001b[0m fid_value \u001b[39m=\u001b[39m calculate_frechet_distance(m1, s1, m2, s2)\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\pytorch_fid\\fid_score.py:243\u001b[0m, in \u001b[0;36mcompute_statistics_of_path\u001b[1;34m(path, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[0;32m    240\u001b[0m     path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(path)\n\u001b[0;32m    241\u001b[0m     files \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m([file \u001b[39mfor\u001b[39;00m ext \u001b[39min\u001b[39;00m IMAGE_EXTENSIONS\n\u001b[0;32m    242\u001b[0m                    \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m path\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m*.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(ext))])\n\u001b[1;32m--> 243\u001b[0m     m, s \u001b[39m=\u001b[39m calculate_activation_statistics(files, model, batch_size,\n\u001b[0;32m    244\u001b[0m                                            dims, device, num_workers)\n\u001b[0;32m    246\u001b[0m \u001b[39mreturn\u001b[39;00m m, s\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\pytorch_fid\\fid_score.py:228\u001b[0m, in \u001b[0;36mcalculate_activation_statistics\u001b[1;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_activation_statistics\u001b[39m(files, model, batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, dims\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m,\n\u001b[0;32m    210\u001b[0m                                     device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[39m\"\"\"Calculation of the statistics used by the FID.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m    Params:\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m    -- files       : List of image files paths\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m               the inception model.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     act \u001b[39m=\u001b[39m get_activations(files, model, batch_size, dims, device, num_workers)\n\u001b[0;32m    229\u001b[0m     mu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(act, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    230\u001b[0m     sigma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(act, rowvar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\DSD\\lib\\site-packages\\pytorch_fid\\fid_score.py:143\u001b[0m, in \u001b[0;36mget_activations\u001b[1;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mif\u001b[39;00m pred\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m pred\u001b[39m.\u001b[39msize(\u001b[39m3\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    141\u001b[0m     pred \u001b[39m=\u001b[39m adaptive_avg_pool2d(pred, output_size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m--> 143\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39;49msqueeze(\u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49msqueeze(\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    145\u001b[0m pred_arr[start_idx:start_idx \u001b[39m+\u001b[39m pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m pred\n\u001b[0;32m    147\u001b[0m start_idx \u001b[39m=\u001b[39m start_idx \u001b[39m+\u001b[39m pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([f\"{cwd}\\celeb_64.npz\", \n",
    "                         f\"{cwd}\\saved_images\\celeb\\DSDGL\\\\8\"], 256, dims=2048, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [01:31<00:00,  1.29it/s]\n",
      "100%|██████████| 118/118 [01:36<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7282996361477387"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([f\"{cwd}\\saved_images\\celeb\\DSDGL\\\\8\", \n",
    "                         f\"{cwd}\\saved_images\\celeb\\TSD\\\\8\"], 256, dims=2048, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for dimension 0 with size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sampler_original\u001b[39m.\u001b[39;49mmake_schedule(ddim_num_steps\u001b[39m=\u001b[39;49m\u001b[39m36\u001b[39;49m, ddim_eta\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Diffusion_Thesis\\cin_256\\ldm\\models\\diffusion\\ddim.py:45\u001b[0m, in \u001b[0;36mDDIMSampler.make_schedule\u001b[1;34m(self, ddim_num_steps, ddim_discretize, ddim_eta, verbose)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m'\u001b[39m\u001b[39msqrt_recipm1_alphas_cumprod\u001b[39m\u001b[39m'\u001b[39m, to_torch(np\u001b[39m.\u001b[39msqrt(\u001b[39m1.\u001b[39m \u001b[39m/\u001b[39m alphas_cumprod\u001b[39m.\u001b[39mcpu() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)))\n\u001b[0;32m     44\u001b[0m \u001b[39m# ddim sampling parameters\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m ddim_sigmas, ddim_alphas, ddim_alphas_prev \u001b[39m=\u001b[39m make_ddim_sampling_parameters(alphacums\u001b[39m=\u001b[39;49malphas_cumprod\u001b[39m.\u001b[39;49mcpu(),\n\u001b[0;32m     46\u001b[0m                                                                            ddim_timesteps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mddim_timesteps,\n\u001b[0;32m     47\u001b[0m                                                                            eta\u001b[39m=\u001b[39;49mddim_eta,verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m'\u001b[39m\u001b[39mddim_sigmas\u001b[39m\u001b[39m'\u001b[39m, ddim_sigmas)\n\u001b[0;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m'\u001b[39m\u001b[39mddim_alphas\u001b[39m\u001b[39m'\u001b[39m, ddim_alphas)\n",
      "File \u001b[1;32mc:\\Diffusion_Thesis\\cin_256\\ldm\\modules\\diffusionmodules\\util.py:69\u001b[0m, in \u001b[0;36mmake_ddim_sampling_parameters\u001b[1;34m(alphacums, ddim_timesteps, eta, verbose)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_ddim_sampling_parameters\u001b[39m(alphacums, ddim_timesteps, eta, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m     \u001b[39m# select alphas for computing the variance schedule\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     alphas \u001b[39m=\u001b[39m alphacums[ddim_timesteps]\n\u001b[0;32m     70\u001b[0m     alphas_prev \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([alphacums[\u001b[39m0\u001b[39m]] \u001b[39m+\u001b[39m alphacums[ddim_timesteps[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m     72\u001b[0m     \u001b[39m# according the the formula provided in https://arxiv.org/abs/2010.02502\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1000 is out of bounds for dimension 0 with size 1000"
     ]
    }
   ],
   "source": [
    "sampler_original.make_schedule(ddim_num_steps=36, ddim_eta=0.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = util.compare_teacher_student_x0(teacher, sampler_teacher, original, sampler_original, steps=[2, 4, 8, 16], prompt=[992])\n",
    "# images, _ = util.compare_teacher_student(teacher, sampler_teacher, original, sampler_original, steps=[2], prompt=[992])\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = util.compare_teacher_student(teacher, sampler_teacher, original, sampler_original, steps=[38], prompt=[992])\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9030899869919435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Administrator/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/CelebA.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\DSD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 274.06 M params.\n",
      "Keeping EMAs of 370.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n",
      "Training LatentDiffusion as an unconditional model.\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 274.06 M params.\n",
      "Keeping EMAs of 370.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n",
      "Training LatentDiffusion as an unconditional model.\n"
     ]
    }
   ],
   "source": [
    "# from util import *\n",
    "# import wandb\n",
    "\n",
    "cwd = os.getcwd()\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!wandb login 4baa24c4fc6c8eed782cacb721d34977149d4fcb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"Cin_256_custom.ipynb\"\n",
    "\n",
    "# config_path=f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "# model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "teacher, sampler_teacher = util.create_models(config_path, model_path, student=False)\n",
    "path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\iterative\\\\2.pt\"\n",
    "student, sampler_student, optimizer, scheduler = util.load_trained(path, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def organize_images(folder_path):\n",
    "    # Get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Loop through each file\n",
    "    for file in files:\n",
    "        # Get the number before the first \"_\" in the filename\n",
    "        number = file.split(\"_\")[0]\n",
    "\n",
    "        # Create a folder with that number if it doesn't exist\n",
    "        if not os.path.exists(os.path.join(folder_path, number)):\n",
    "            os.mkdir(os.path.join(folder_path, number))\n",
    "\n",
    "        # Move the file to that folder\n",
    "        shutil.move(os.path.join(folder_path, file), os.path.join(folder_path, number, file))\n",
    "\n",
    "organize_images(f\"{cwd}/saved_images/8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception score: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.inception_v3(pretrained=True)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def preprocess(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(img)\n",
    "    # img_tensor = img_tensor.unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_inception_score(model, fake_images, batch_size=32, splits=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    scores = []\n",
    "    dataloader = data.DataLoader(fake_images, batch_size=batch_size)\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = F.softmax(model(batch), dim=1).cpu().numpy()\n",
    "            scores.append(pred)\n",
    "\n",
    "    scores = np.concatenate(scores, axis=0)\n",
    "    kl_divs = []\n",
    "    chunk_size = len(scores) // splits\n",
    "\n",
    "    for i in range(splits):\n",
    "        chunk = scores[i*chunk_size:(i+1)*chunk_size, :]\n",
    "        mean_probs = np.mean(chunk, axis=0)\n",
    "        kl_div = entropy(mean_probs, qk=np.mean(chunk, axis=0), base=2)\n",
    "        kl_divs.append(kl_div)\n",
    "\n",
    "    return np.exp(np.mean(kl_divs))\n",
    "\n",
    "inception_score = calculate_inception_score(model=model, fake_images=trained)\n",
    "print(\"Inception score:\", inception_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{cwd}/saved_images/8\"\n",
    "inception_score(trained)\n",
    "\n",
    "inception.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_path = \"C:\\Diffusion_Thesis\\cin_256\\saved_images\\\\trained_unit_4\\8\"\n",
    "original_path = \"C:\\Diffusion_Thesis\\cin_256\\saved_images\\\\original_unit_4\\8\"\n",
    "# original_path = \"C:\\Diffusion_Thesis\\cin_256\\saved_images\\original_scheduler\\\\64\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            if img.mode == 'RGB':\n",
    "                transform = transforms.Compose([transforms.ToTensor()])\n",
    "                # tensor_img = transform(img)\n",
    "                tensor_img = preprocess(img)\n",
    "                # tensor_img = (tensor_img * 255).type(torch.uint8)\n",
    "                images.append(tensor_img)\n",
    "    tensor_images = torch.stack(images)\n",
    "    return tensor_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = load_images_from_folder(trained_path)\n",
    "original = load_images_from_folder(original_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.inception import InceptionScore\n",
    "inception = InceptionScore(feature=2048)\n",
    "inception.update(original)\n",
    "inception.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0756), tensor(0.0479))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.image.inception import InceptionScore\n",
    "inception = InceptionScore(feature=2048)\n",
    "inception.update(trained)\n",
    "inception.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher-Student Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim_steps = [64]\n",
    "run_name = \"unconditional\"\n",
    "generations = 100000\n",
    "lr=0.00000001\n",
    "config = f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "original_model_path = f\"{cwd}/models/cin256_original.ckpt\"\n",
    "distill(ddim_steps, generations, run_name, config, original_model_path, lr=lr, start_trained=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create teacher and student model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the student or teacher:\n",
    "\n",
    "(setting student=False will only return a single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "teacher, sampler_teacher = util.create_models(config_path, model_path, student=False)\n",
    "# student, sampler_student = create_models(config_path, model_path, student=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\n",
      "Extracting features from input1\n",
      "Looking for samples non-recursivelty in \"C:\\imagenet\\DIFFERENCE\\fill\" with extensions png,jpg,jpeg\n",
      "Found 2000 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                           \n",
      "Extracting features from input2\n",
      "Looking for samples non-recursivelty in \"C:\\imagenet\\DIFFERENCE\\fill\" with extensions png,jpg,jpeg\n",
      "Found 2000 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                           \n",
      "Inception Score: 87.56326427270122 ± 5.9974222382967906\n",
      "Frechet Inception Distance: -9.095103905565338e-07\n",
      "Kernel Inception Distance: -0.00032915234734734786 ± 4.328567493067871e-05       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inception_score_mean': 87.56326427270122,\n",
       " 'inception_score_std': 5.9974222382967906,\n",
       " 'frechet_inception_distance': -9.095103905565338e-07,\n",
       " 'kernel_inception_distance_mean': -0.00032915234734734786,\n",
       " 'kernel_inception_distance_std': 4.328567493067871e-05}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_fidelity\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "torch_fidelity.calculate_metrics(gpu=0, fid=True, isc=True, kid=True, input1=r\"C:\\imagenet\\DIFFERENCE\\fill\", input2=r\"C:\\imagenet\\DIFFERENCE\\fill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.dataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher-Student Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing trained student with Teacher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:28<00:00,  3.37it/s]\n",
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n",
      "100%|██████████| 500/500 [06:35<00:00,  1.27it/s]\n",
      "100%|██████████| 32/32 [00:16<00:00,  1.91it/s]\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.55it/s]\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184.69918112140454, 90.90177272994146, 86.31800167274423]\n"
     ]
    }
   ],
   "source": [
    "# from util import *\n",
    "# import wandb\n",
    "import self_distillation\n",
    "import saving_loading\n",
    "import generate\n",
    "import wandb\n",
    "import util\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "cwd = os.getcwd()\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!wandb login 4baa24c4fc6c8eed782cacb721d34977149d4fcb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"Cin_256_custom.ipynb\"\n",
    "config_path=f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "\n",
    "# config_path=f\"{cwd}/models/configs/celebahq-ldm-vq-4.yaml\"\n",
    "# model_path=f\"{cwd}/models/CelebA.ckpt\"\n",
    "\n",
    "# model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "\n",
    "\n",
    "\n",
    "# teacher_path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\TSD\\TSD_cin_50k_1e8\\\\4.pt\"\n",
    "# teacher, sampler_teacher = util.create_models(config_path, teacher_path, student=False)\n",
    "\n",
    "\n",
    "# FINALS:\n",
    "\n",
    "\n",
    "\n",
    "# # 4000 VERSIONS:\n",
    "# path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\gradual_exp\\cin_DSDGEXP_64_1e-07_4000\\\\1.pt\"\n",
    "# path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\gradual_linear\\cin_DSDGL_64_1e-07_4000\\\\1.pt\"\n",
    "# path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\iterative\\cin_DSDI_64_1e-07_4000\\\\2.pt\"\n",
    "# path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_previous\\\\final_versions\\cin\\DSDI\\\\2.pt\"\n",
    "# # path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\\\naive\\cin_DSDN_64_1e-07_4000\\\\32.pt\"\n",
    "# # path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_previous\\\\final_versions\\cin\\DSDN\\\\32.pt\"\n",
    "# # path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\TSD\\cin_TSD_64_1e-07_4000\\\\8.pt\"\n",
    "# # path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_previous\\\\final_versions\\cin\\TSD\\\\4.pt\"\n",
    "# # path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\TSD\\cin_TSD_64_1e-07_4000\\\\2.pt\"\n",
    "# # path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\gradual_linear\\cin_DSDGL_64_1e-07_4000\\\\1.pt\"\n",
    "# # path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_previous\\\\final_versions\\cin\\DSDGL\\\\1.pt\"\n",
    "# # path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\gradual_linear\\cin_DSDGL_64_1e-07_4000\\\\1.pt\"\n",
    "tsd = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\\\final_versions\\cin\\TSD\\\\16.pt\"\n",
    "dsdn = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\\\naive\\cin_DSDN_64_1e-07_4000\\\\32.pt\"\n",
    "dsdgl = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\gradual_linear\\cin_DSDGL_64_1e-07_4000\\\\1.pt\"\n",
    "dsdgexp = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\gradual_exp\\cin_DSDGEXP_64_1e-07_4000\\\\1.pt\"\n",
    "dsdi = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\\\iterative\\cin_DSDI_64_1e-07_4000\\\\2.pt\"\n",
    "\n",
    "\n",
    "student, sampler_student, optimizer, scheduler = util.load_trained(dsdi, config_path)\n",
    "del optimizer, scheduler\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "fid = util.get_fid(student, sampler_student, num_imgs=500, name=\"temp_cin_DSDI\", instance=2, steps=[2, 4, 8])\n",
    "print(fid)\n",
    "del student, sampler_student\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# model_path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_previous\\\\final_versions\\celeb\\TSD\\\\2.pt\"\n",
    "\n",
    "# long, sampler_long, optimizer, scheduler = util.load_trained(model_path, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path=f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "\n",
    "path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\iterative\\cin_DSDI_64_1e-07_4000\\\\2.pt\"\n",
    "student, sampler_student, optimizer, scheduler = util.load_trained(path, config_path)\n",
    "\n",
    "\n",
    "new_image, noise_list, prompt_list = generate.ablation(sampler_student, steps=8, shape=(1, 6), celeb=False, model_type=\"DSDI\")\n",
    "\n",
    "\n",
    "del optimizer, scheduler, student, sampler_student\n",
    "torch.cuda.empty_cache()\n",
    "new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path=f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "\n",
    "path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models_4000\\gradual_exp\\cin_DSDGEXP_64_1e-07_4000\\\\1.pt\"\n",
    "student, sampler_student, optimizer, scheduler = util.load_trained(path, config_path)\n",
    "\n",
    "\n",
    "new_image, _, _ = generate.ablation(sampler_student, steps=8, shape=(1, 6), celeb=False, model_type=\"DSDGEXP\", noise_list=noise_list, prompt_list=prompt_list)\n",
    "\n",
    "\n",
    "del optimizer, scheduler, student, sampler_student\n",
    "torch.cuda.empty_cache()\n",
    "new_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image, noise_list, prompt_list = generate.ablation(sampler_student, steps=8, shape=(3, 3), celeb=False)\n",
    "new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image, noise_list, prompt_list = generate.ablation(sampler_student, steps=4, shape=(3, 3), noise_list=noise_list, prompt_list=prompt_list, celeb=True)\n",
    "new_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.000/100.000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image, noise_list, prompt_list = generate.ablation(sampler_long, steps=8, shape=(3, 3), noise_list=noise_list, prompt_list=prompt_list, celeb=True)\n",
    "new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image, noise_list, prompt_list = generate.ablation(sampler_long, steps=4, shape=(3, 3), noise_list=noise_list, prompt_list=prompt_list, celeb=True)\n",
    "new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(a)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:09<00:00,  2.01it/s]\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[99.57427079842358]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_fid import fid_score\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "util.get_fid_celeb(student, sampler_student, num_imgs=500, name=\"DSDN\", instance=8, steps=[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:16<00:00,  1.95it/s]\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[107.01830823393303]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_fid import fid_score\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "util.get_fid_celeb(original, sampler_original, num_imgs=500, name=\"DSDGEXP\", instance=8, steps=[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [35:56<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "util.save_images(student, sampler_student, 5000, \"TSD\", [4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "original, sampler_original = util.create_models(config_path, model_path, student=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = util.compare_teacher_student_celeb(original, sampler_original, student, sampler_student, steps=[8, 4, 2])\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = util.compare_teacher_student(original, sampler_original, student, sampler_student, steps=[32, 16,8, 4, 2], prompt=992)\n",
    "images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITHOUT schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, x_T_copy, class_prompt, intermediates  = generate.generate_images(student, sampler_student, steps=4, scale=3, keep_intermediates=False,\n",
    "                                            x_0=False, class_prompt=992)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, x_T_copy, class_prompt, intermediates  = generate.generate_images(teacher, sampler_teacher, steps=4, scale=3, keep_intermediates=False,\n",
    "                                            x_0=False, class_prompt=992)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images, _ = util.compare_teacher_student(teacher, sampler_teacher, student, sampler_student, steps=[32, 16,8, 4, 2, 1], prompt=992)\n",
    "# images, _ = util.compare_teacher_student(teacher, sampler_teacher, student, sampler_student, steps=[1, 2, 4], prompt=992)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [05:23<00:00,  1.54it/s]\n",
      "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[87.65390030232533]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.get_fid(student, sampler_student, num_imgs=500, name=\"naive_best\", instance=4, steps=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_fid(teacher, sampler_teacher, num_imgs=500, name=\"tsd_best\", instance=4, steps=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:05<00:00,  6.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[103.82042951118473]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.get_fid(original, sampler_original, num_imgs=500, name=\"original_500\", instance=4, steps=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:43<00:00,  2.24it/s]\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[276.57968163823114]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.get_fid(student, sampler_student, num_imgs=500, name=\"cin_50k_noschedule_x0\", instance=4, steps=[4], x_0=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:45<00:00,  2.22it/s]\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[351.54266565679205]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.get_fid(teacher, sampler_teacher, num_imgs=500, name=\"original_test_x0\", instance=4, steps=[4],x_0=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Administrator/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\DSD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "import self_distillation\n",
    "import saving_loading\n",
    "import generate\n",
    "import wandb\n",
    "import util\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!wandb login 4baa24c4fc6c8eed782cacb721d34977149d4fcb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"Cin_256_custom.ipynb\"\n",
    "\n",
    "config_path=f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "\n",
    "# config_path=f\"{cwd}/models/configs/celebahq-ldm-vq-4.yaml\"\n",
    "# model_path=f\"{cwd}/models/CelebA.ckpt\"\n",
    "\n",
    "teacher, sampler_teacher = util.create_models(config_path, model_path, student=False)\n",
    "original, sampler_original = util.create_models(config_path, model_path, student=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_scheduler = \"naive\"\n",
    "model_NAME = \"CIN256_weirdLR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling to: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:44<00:00, 10.96s/it, epoch_loss=0.00266]\n"
     ]
    }
   ],
   "source": [
    "steps = 64\n",
    "generations = 5000\n",
    "lr = 0.000001\n",
    "decrease_steps = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_name = f\"HOME_{model_NAME}-DSD-{step_scheduler}-{steps}-{lr}-{generations}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "notes = f\"\"\"Self-distilling the {steps} step original teacher into a smaller continuous model,\"\"\"\n",
    "# wandb_session = util.wandb_log(name=run_name, lr=lr, model=teacher, tags=[\"DSD Iterative\", run_name, model_NAME], notes=notes)\n",
    "# wandb.run.log_code(\".\")\n",
    "# session = wandb_session\n",
    "session=None\n",
    "optimizer, scheduler = util.get_optimizer(sampler_teacher, iterations=generations, lr=lr)\n",
    "\n",
    "self_distillation.self_distillation_CIN(teacher, sampler_teacher, original, sampler_original, optimizer=optimizer, scheduler=scheduler, session=session, \n",
    "                      steps=steps, generations=generations, early_stop=False, \n",
    "                      run_name=run_name, decrease_steps=decrease_steps, step_scheduler=step_scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, _ = util.compare_teacher_student(original, sampler_original, teacher, sampler_teacher, steps=[32, 16,8, 4, 2], prompt=992)\n",
    "# images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = util.compare_teacher_student(original, sampler_original, teacher, sampler_teacher, steps=[32, 16,8, 4, 2], prompt=992)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # images, _ = util.compare_teacher_student(original, sampler_original, teacher, sampler_teacher, steps=[4], prompt=992)\n",
    "# images, _ = util.compare_teacher_student_x0(original, sampler_original, teacher, sampler_teacher, steps=[32, 16,8, 4, 2], prompt=992)\n",
    "# # images, _ = util.compare_teacher_student_celeb(original, sampler_original, teacher, sampler_teacher, steps=[64, 32, 16, 8,  4, 2, 1])\n",
    "# images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, _ = util.compare_teacher_student(original, sampler_original, teacher, sampler_teacher, steps=[4], prompt=992)\n",
    "images, _ = util.compare_teacher_student_x0(original, sampler_original, teacher, sampler_teacher, steps=[32, 16,8, 4, 2], prompt=992)\n",
    "# images, _ = util.compare_teacher_student_celeb(original, sampler_original, teacher, sampler_teacher, steps=[64, 32, 16, 8,  4, 2, 1])\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "sampler_teacher.make_schedule(ddim_num_steps=64, ddim_eta=0.0, verbose=False)\n",
    "xc = torch.tensor([992])\n",
    "c_teacher = teacher.get_learned_conditioning({teacher.cond_stage_key: xc.to(teacher.device)})\n",
    "sc = teacher.get_learned_conditioning(\n",
    "                            {teacher.cond_stage_key: torch.tensor(1*[1000]).to(teacher.device)}\n",
    "                            )\n",
    "temp = []\n",
    "samples_ddim = None\n",
    "for step in range(64):\n",
    "    samples_ddim, teacher_intermediate, x_T, pred_x0_teacher, a_t_teacher = sampler_teacher.sample(S=1,\n",
    "                                                                        conditioning=c_teacher,\n",
    "                                                                        batch_size=1,\n",
    "                                                                        shape=[3, 64, 64],\n",
    "                                                                        verbose=False,\n",
    "                                                                        x_T=samples_ddim,\n",
    "                                                                        \n",
    "                                                                        unconditional_guidance_scale=3.0,\n",
    "                                                                        unconditional_conditioning=sc, \n",
    "                                                                        eta=0.0,\n",
    "                                                                        keep_intermediates=False,\n",
    "                                                                        intermediate_step =step,\n",
    "                                                                        steps_per_sampling = 1,\n",
    "                                                                        total_steps = 64)\n",
    "    \n",
    "    x_T_teacher_decode = sampler_teacher.model.decode_first_stage(pred_x0_teacher)\n",
    "    teacher_target = torch.clamp((x_T_teacher_decode+1.0)/2.0, min=0.0, max=1.0)\n",
    "    temp.append(teacher_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, grid = util.compare_latents(temp)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.save_model(sampler_teacher, optimizer, scheduler, name=step_scheduler, steps=1, run_name=run_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "util.save_images(teacher, sampler_teacher, 50000, \"original\", [64, 32, 16, 8, 4, 2, 1])\n",
    "# util.save_images(teacher, sampler_teacher, 100, \"original_unit_4\", [8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:16<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "util.save_images(student, sampler_student, 100, \"TSD\", [4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_fid(teacher, sampler_teacher, num_imgs=1000, name=\"original_unit_4\", instance = 4, steps=[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_fid(student, sampler_student, num_imgs=500, name=\"trained_unit_4\", instance = 4, steps=[8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:33<00:00,  2.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34.22429355433479"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_fid import fid_score\n",
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", \"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDGL\\\\4\"],batch_size=64,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid import fid_score\n",
    "# path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\\\naive\\cin_50k_1e-8\\\\32.pt\"\n",
    "path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\gradual_linear\\DSDGL_cin_50k_4e-9\\\\1.pt\"\n",
    "path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\\\naive\\cin_50k_1e-8_noschedule\\\\32.pt\"\n",
    "# path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\gradual_exp\\DSDGEXP_cin_50k_4e-9\\\\1.pt\"\n",
    "path = \"C:\\Diffusion_Thesis\\cin_256\\data\\\\trained_models\\\\TSD\\TSD_cin_50k_1e8\\\\2.pt\"\n",
    "\n",
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", \"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDGEXP\\\\4\"],batch_size=64,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:35<00:00,  2.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35.14156308740212"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", \"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDGEXP\\\\4\"],batch_size=64,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:31<00:00,  2.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.810607324341618"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", \"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDI\\\\4\"],batch_size=64,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:31<00:00,  2.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.159559182758358"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", \"C:\\Diffusion_Thesis\\cin_256\\saved_images\\TSD\\\\4\"],batch_size=64,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WITHOUT intermediates saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, x_T_copy, class_prompt, intermediates  = generate.generate_images(teacher, sampler_teacher, steps=16, scale=3, keep_intermediates=False,\n",
    "                                            x_0=True, class_prompt=992)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WITH intermediates saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediates, starting_noise, class_prompt = return_intermediates_for_student(teacher, sampler_teacher, steps=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_to_img(teacher, intermediates[125])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:26:03<00:00, 10.33s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = f\"{cwd}/data/pre_generated/teacher_128/\"\n",
    "dataset_name = \"teacher_128.pt\"\n",
    "make_dataset(teacher, sampler_teacher, 501, 128, dataset_path, dataset_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train student from pre-generated teacher dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"{cwd}/data/pre_generated/teacher_128/\"\n",
    "datasets = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = \"\"\"This is a serious attempt to distill the 128 step original teacher into a 64 step student, trained on 32000 instances\"\"\"\n",
    "wandb_session = wandb_log(name=\"Train_student_on_128_pretrained\", lr=0.00000001, model=student, tags=[\"distillation\"], notes=notes)\n",
    "\n",
    "optimizer, scheduler = get_optimizer(sampler_student, iterations=64*500)\n",
    "\n",
    "dataset = torch.load(data_dir + datasets[0])\n",
    "train_student_from_dataset(student, sampler_student, dataset, 64, optimizer, scheduler, early_stop=True, session=wandb_session)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "save_model(sampler_student, optimizer, scheduler, name=\"lr8_scheduled\", steps=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Loading trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(sampler_student, optimizer, scheduler, name=\"lr8_scheduled\", steps=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{cwd}/data/trained_models/64/student_lr8_scheduled.pt\"\n",
    "student, sampler_student, optimizer, scheduler = util.load_trained(path, config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = util.compare_teacher_student(teacher, sampler_teacher, student, sampler_student, steps=[1, 2, 4, 8, 16, 32, 64, 128])\n",
    "images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "gradient_updates = 1000\n",
    "halving_steps = []\n",
    "for i in range(math.floor(math.log(32)/math.log(2))+1):\n",
    "    halving_steps.append(int(gradient_updates * (1 / (math.floor(math.log(32)/math.log(2))+1)) * i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "halvings = math.floor(math.log(32)/math.log(2)) + 1\n",
    "halvings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Administrator/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\DSD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loading model from c:\\Diffusion_Thesis\\cin_256/models/cin256_original.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 400.92 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "import self_distillation\n",
    "import saving_loading\n",
    "import generate\n",
    "import wandb\n",
    "import util\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!wandb login 4baa24c4fc6c8eed782cacb721d34977149d4fcb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"Cin_256_custom.ipynb\"\n",
    "\n",
    "config_path=f\"{cwd}/models/configs/cin256-v2-custom.yaml\"\n",
    "model_path=f\"{cwd}/models/cin256_original.ckpt\"\n",
    "\n",
    "# config_path=f\"{cwd}/models/configs/celebahq-ldm-vq-4.yaml\"\n",
    "# model_path=f\"{cwd}/models/CelebA.ckpt\"\n",
    "\n",
    "original, sampler_original = util.create_models(config_path, model_path, student=False)\n",
    "student, sampler_student = util.create_models(config_path, model_path, student=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrease_steps = True\n",
    "optimizer, scheduler = util.get_optimizer(sampler_student, iterations=400, lr=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling to: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:17<00:00,  4.27s/it, epoch_loss=0.00876]\n"
     ]
    }
   ],
   "source": [
    "gradient_updates = generations\n",
    "ddim_steps_student = steps\n",
    "TEACHER_STEPS = 2\n",
    "ddim_eta = 0.0\n",
    "scale = 3.0\n",
    "optimizer=optimizer\n",
    "averaged_losses = []\n",
    "criterion = nn.MSELoss()\n",
    "instance = 0\n",
    "generation = 0\n",
    "all_losses = []\n",
    "\n",
    "if step_scheduler == \"iterative\":\n",
    "    halvings = math.floor(math.log(32)/math.log(2)) + 1\n",
    "    updates_per_halving = int(gradient_updates / halvings)\n",
    "intermediate_generation_compare = int(gradient_updates * 0.8 / 10)\n",
    "if step_scheduler == \"FID\":\n",
    "    if os.path.exists(f\"{cwd}/saved_images/FID/{run_name}\"):\n",
    "        print(\"FID folder exists\")\n",
    "        shutil.rmtree(f\"{cwd}/saved_images/FID/{run_name}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    with student.ema_scope():              \n",
    "            \n",
    "            sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "            sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "            \n",
    "            \n",
    "            if step_scheduler ==\"FID\":\n",
    "                current_fid = util.get_fid(student, sampler_student, num_imgs=100, name=run_name, instance = 0, steps=[ddim_steps_student])\n",
    "            \n",
    "            updates = ddim_steps_student\n",
    "\n",
    "            for i in range(halvings):\n",
    "                if instance != 0:\n",
    "                    util.save_model(sampler_student, optimizer, scheduler, name=step_scheduler, steps=updates, run_name=run_name)\n",
    "                updates = int(updates / TEACHER_STEPS)\n",
    "                generations = updates_per_halving // updates\n",
    "                print(\"Distilling to:\", updates)\n",
    "                \n",
    "                \n",
    "                with tqdm.tqdm(range(generations)) as tepoch:\n",
    "\n",
    "                    for i in tepoch:\n",
    "                        generation += 1\n",
    "                        losses = []        \n",
    "            \n",
    "                        samples_ddim= None\n",
    "                        predictions_temp = []\n",
    "                        for steps in range(updates):  \n",
    "\n",
    "                                with autocast():\n",
    "\n",
    "                                    with torch.enable_grad():\n",
    "                                        \n",
    "                                        instance += 1\n",
    "                                    \n",
    "                                        \n",
    "                                        \n",
    "                                        optimizer.zero_grad()\n",
    "                                        \n",
    "                                        \n",
    "                                        samples_ddim, pred_x0_student, _, at= sampler_student.sample_student(S=1,\n",
    "                                                                            conditioning=None,\n",
    "                                                                            batch_size=1,\n",
    "                                                                            shape=[3, 64, 64],\n",
    "                                                                            verbose=False,\n",
    "                                                                            x_T=samples_ddim,\n",
    "                                                                    \n",
    "                                                                            unconditional_guidance_scale=scale,\n",
    "                                                                            unconditional_conditioning=None, \n",
    "                                                                            eta=ddim_eta,\n",
    "                                                                            keep_intermediates=False,\n",
    "                                                                            intermediate_step = steps*2,\n",
    "                                                                            steps_per_sampling = 1,\n",
    "                                                                            total_steps = updates*2)\n",
    "                                    \n",
    "\n",
    "                                    with torch.no_grad():\n",
    "                                        \n",
    "                                        samples_ddim, _, _, pred_x0_teacher, _ = sampler_student.sample(S=1,\n",
    "                                                                        conditioning=None,\n",
    "                                                                        batch_size=1,\n",
    "                                                                        shape=[3, 64, 64],\n",
    "                                                                        verbose=False,\n",
    "                                                                        x_T=samples_ddim,\n",
    "                                                                        unconditional_guidance_scale=scale,\n",
    "                                                                        unconditional_conditioning=None, \n",
    "                                                                        eta=ddim_eta,\n",
    "                                                                        keep_intermediates=False,\n",
    "                                                                        intermediate_step = steps*2+1,\n",
    "                                                                        steps_per_sampling = 1,\n",
    "                                                                        total_steps = updates*2)     \n",
    "                                    \n",
    "                                    \n",
    "                                \n",
    "                                    with torch.enable_grad():    \n",
    "                                        if type == \"home\":\n",
    "                                            # AUTOCAST:\n",
    "                                            signal = at\n",
    "                                            noise = 1 - at\n",
    "                                            log_snr = torch.log(signal / noise)\n",
    "                                            weight = max(log_snr, 1)\n",
    "                                            loss = weight * criterion(pred_x0_student, pred_x0_teacher.detach())\n",
    "                                            scaler.scale(loss).backward()\n",
    "                                            scaler.step(optimizer)\n",
    "                                            scaler.update()\n",
    "                                            # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                            \n",
    "                                            scheduler.step()\n",
    "                                            losses.append(loss.item())\n",
    "\n",
    "                                        else:\n",
    "                                            # NO AUTOCAST:\n",
    "                                            signal = at\n",
    "                                            noise = 1 - at\n",
    "                                            log_snr = torch.log(signal / noise)\n",
    "                                            weight = max(log_snr, 1)\n",
    "                                            loss = weight * criterion(pred_x0_student, pred_x0_teacher.detach())\n",
    "                                            loss.backward()\n",
    "                                            optimizer.step()\n",
    "                                            scheduler.step()\n",
    "                                            # torch.nn.utils.clip_grad_norm_(sampler_student.model.parameters(), 1)\n",
    "                                            \n",
    "                                            losses.append(loss.item())\n",
    "                                        \n",
    "                                    if session != None and generation % 100 == 0 and generation > 0:\n",
    "                                            \n",
    "                                        x_T_teacher_decode = sampler_student.model.decode_first_stage(pred_x0_teacher)\n",
    "                                        teacher_target = torch.clamp((x_T_teacher_decode+1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        x_T_student_decode = sampler_student.model.decode_first_stage(pred_x0_student.detach())\n",
    "                                        student_target  = torch.clamp((x_T_student_decode +1.0)/2.0, min=0.0, max=1.0)\n",
    "                                        predictions_temp.append(teacher_target)\n",
    "                                        predictions_temp.append(student_target)\n",
    "                                        \n",
    "                                \n",
    "\n",
    "                                    # if session != None and instance % 10000 == 0 and generation > 0:\n",
    "                                    #     fids = util.get_fid_celeb(student, sampler_student, num_imgs=100, name=run_name, instance = instance+1, steps=[64, 32, 16, 8, 4, 2, 1])\n",
    "                                    #     session.log({\"fid_64\":fids[0]})\n",
    "                                    #     session.log({\"fid_32\":fids[1]})\n",
    "                                    #     session.log({\"fid_16\":fids[2]})\n",
    "                                    #     session.log({\"fid_8\":fids[3]})\n",
    "                                    #     session.log({\"fid_4\":fids[4]})\n",
    "                                    #     session.log({\"fid_2\":fids[5]})\n",
    "                                    #     session.log({\"fid_1\":fids[6]})\n",
    "                                    \n",
    "                                    if session != None and instance % 2000 == 0:\n",
    "                                \n",
    "                                        # with torch.no_grad():\n",
    "                                        #     images, _ = util.compare_teacher_student_celeb(original, sampler_original, student, sampler_student, steps=[64, 32, 16, 8,  4, 2, 1])\n",
    "                                        #     images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                                        #     wandb.log({\"pred_x0\": images})\n",
    "                                        #     # images, _ = util.compare_teacher_student_with_schedule(original, sampler_original, student, sampler_student, steps=[64, 32, 16, 8,  4, 2, 1], prompt=992)\n",
    "                                        #     # images = wandb.Image(_, caption=\"left: Teacher, right: Student\")\n",
    "                                        #     # wandb.log({\"schedule\": images})\n",
    "                                        #     sampler_student.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "                                        #     sampler_original.make_schedule(ddim_num_steps=ddim_steps_student, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "                        # if generation > 0 and generation % 20 == 0 and ddim_steps_student != 1 and step_scheduler==\"FID\":\n",
    "                        #     fid = util.get_fid(student, sampler_student, num_imgs=100, name=run_name, \n",
    "                        #                 instance = instance, steps=[ddim_steps_student])\n",
    "                        #     if fid[0] <= current_fid[0] * 0.9 and decrease_steps==True:\n",
    "                        #         print(fid[0], current_fid[0])\n",
    "                        #         if ddim_steps_student in [16, 8, 4, 2, 1]:\n",
    "                        #             name = \"intermediate\"\n",
    "                        #             saving_loading.save_model(sampler_student, optimizer, scheduler, name, steps * 2, run_name)\n",
    "                        #         if ddim_steps_student != 2:\n",
    "                        #             ddim_steps_student -= 2\n",
    "                        #             updates -= 1\n",
    "                        #         else:\n",
    "                        #             ddim_steps_student = 1\n",
    "                        #             updates = 1    \n",
    "                        #         current_fid = fid\n",
    "                        #         print(\"steps decresed:\", ddim_steps_student)    \n",
    "\n",
    "                        if session != None:\n",
    "                            with torch.no_grad():\n",
    "                                if session != None and generation % 100 == 0 and generation > 0:\n",
    "                                    img, grid = util.compare_latents(predictions_temp)\n",
    "                                    images = wandb.Image(grid, caption=\"left: Teacher, right: Student\")\n",
    "                                    wandb.log({\"Inter_Comp\": images})\n",
    "                                    del img, grid, predictions_temp, x_T_student_decode, x_T_teacher_decode, student_target, teacher_target\n",
    "                                    torch.cuda.empty_cache()\n",
    "                        \n",
    "                        all_losses.extend(losses)\n",
    "                        averaged_losses.append(sum(losses) / len(losses))\n",
    "                        if session != None:\n",
    "                            session.log({\"generation_loss\":averaged_losses[-1]})\n",
    "                        tepoch.set_postfix(epoch_loss=averaged_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid import fid_score\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# 15K\n",
    "dsdgl_2 = r\"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDGL\\2\"\n",
    "dsdgl_4 = r\"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDGL\\4\"\n",
    "dsdi_2 = r\"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDI\\2\"\n",
    "dsdi_4 = r\"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDI\\4\"\n",
    "dsdn_2 = r\"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDN\\2\"\n",
    "dsdn_4 = r\"C:\\Diffusion_Thesis\\cin_256\\saved_images\\DSDN\\4\"\n",
    "tsd_4 = r\"C:\\Diffusion_Thesis\\cin_256\\saved_images\\TSD\\4\"\n",
    "\n",
    "# fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", dsdgl_2],batch_size=64,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.05662442766868"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", dsdgl_4],batch_size=512,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", dsdi_2],batch_size=512,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.90103600910777"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", dsdi_4],batch_size=512,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:32<00:00,  3.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107.05470717580408"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", dsdn_4],batch_size=512,device=\"cuda\", dims=2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:20<00:00,  2.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21.793241893661502"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_score.calculate_fid_given_paths([\"C:/val_saved/real_fid_both.npz\", tsd_4],batch_size=512,device=\"cuda\", dims=2048 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
